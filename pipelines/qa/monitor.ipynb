{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment and Monitor\n",
    "\n",
    "This notebook deploy a model trained in pipeline, and monitor it.\n",
    "\n",
    "## Update Model Package Approval Status\n",
    "\n",
    "We can approve the model using the SageMaker Studio UI or programmatically as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'qa-pipeline-16323001711632300171'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive Model From Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n",
      "[{'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/67ot3yw4s5kf',\n",
      "  'PipelineExecutionDisplayName': 'nidome',\n",
      "  'PipelineExecutionStatus': 'Succeeded',\n",
      "  'StartTime': datetime.datetime(2021, 9, 22, 9, 27, 48, 609000, tzinfo=tzlocal())},\n",
      " {'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/vqiccs4mntb5',\n",
      "  'PipelineExecutionDisplayName': 'execution-1632300253350',\n",
      "  'PipelineExecutionStatus': 'Succeeded',\n",
      "  'StartTime': datetime.datetime(2021, 9, 22, 8, 44, 13, 274000, tzinfo=tzlocal())}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status == \"Executing\":\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "        pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "    #        print('Executions for our pipeline...')\n",
    "    #        print(pipeline_execution_status)\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(executions_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/67ot3yw4s5kf\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_arn = executions_response[0][\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PipelineExecutionSteps': [{'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 980000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model/pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 370000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'CreateQAModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 187000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model-package/qamodelpackagegroup/7'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 298000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'QARegisterModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 675000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'Condition': {'Outcome': 'True'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 375000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'IntentAndSlotCondition',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 55, 910000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-evaluatemodel-d7wij9sxb2'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 18, 202000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'EvaluateModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'CacheHitResult': {'SourcePipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/vqiccs4mntb5'},\n",
      "                             'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 630000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:training-job/pipelines-vqiccs4mntb5-train-jdmj5liqyt'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 256000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'Train',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 16, 867000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-processing-jtj2dbws96'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 27, 49, 711000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'Processing',\n",
      "                             'StepStatus': 'Succeeded'}],\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '1642',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 24 Sep 2021 06:21:52 GMT',\n",
      "                                      'x-amzn-requestid': 'dd666965-5d55-4e0c-b58c-2a2804b7524d'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'dd666965-5d55-4e0c-b58c-2a2804b7524d',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)\n",
    "\n",
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Registered Model and Update Model Approval Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:model-package/qamodelpackagegroup/7\n"
     ]
    }
   ],
   "source": [
    "for execution_step in steps[\"PipelineExecutionSteps\"]:\n",
    "    if execution_step[\"StepName\"] == \"QARegisterModel\":\n",
    "        model_package_arn = execution_step[\"Metadata\"][\"RegisterModel\"][\"Arn\"]\n",
    "        break\n",
    "print(model_package_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_response = sm.update_model_package(\n",
    "    ModelPackageArn=model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",  # Other options are Rejected and PendingManualApproval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:model/pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg\n",
      "created_model_name: pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg\n"
     ]
    }
   ],
   "source": [
    "for execution_step in steps[\"PipelineExecutionSteps\"]:\n",
    "    if execution_step[\"StepName\"] == \"CreateQAModel\":\n",
    "        model_arn = execution_step[\"Metadata\"][\"Model\"][\"Arn\"]\n",
    "        break\n",
    "print(model_arn)\n",
    "\n",
    "created_model_name = model_arn.split(\"/\")[-1]\n",
    "print('created_model_name:', created_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Endpoint from Model Registry and Configure It to Capture Requests\n",
    "\n",
    "### Create model from registry\n",
    "\n",
    "More details here: https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-deploy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from registry name : qa-model-from-registry-1632465588\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "model_from_registry_name = \"qa-model-from-registry-{}\".format(timestamp)\n",
    "print(\"Model from registry name : {}\".format(model_from_registry_name))\n",
    "\n",
    "model_registry_package_container = {\n",
    "    \"ModelPackageName\": model_package_arn,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:093729152554:model/qa-model-from-registry-1632465588',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '95',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 24 Sep 2021 06:39:49 GMT',\n",
      "                                      'x-amzn-requestid': 'b922cc61-ab4e-4cd4-b067-2212e5acfa89'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'b922cc61-ab4e-4cd4-b067-2212e5acfa89',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "create_model_from_registry_respose = sm.create_model(\n",
    "    ModelName=model_from_registry_name, ExecutionRoleArn=role, PrimaryContainer=model_registry_package_container\n",
    ")\n",
    "pprint(create_model_from_registry_respose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-east-1:093729152554:model/qa-model-from-registry-1632465588'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_from_registry_arn = create_model_from_registry_respose[\"ModelArn\"]\n",
    "model_from_registry_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Endpoint to Capture Data from Requests and Responses\n",
    "\n",
    "Check API for [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_bucket = 'sm-nlp-data'\n",
    "data_capture_prefix = 'inference/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an endpoint configuration that Amazon SageMaker hosting services uses to deploy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa-model-from-registry-epc-1632465588\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"qa-model-from-registry-epc-{}\".format(timestamp)\n",
    "print(endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": created_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    "    DataCaptureConfig={\n",
    "        'EnableCapture': True,\n",
    "        'InitialSamplingPercentage': 100,\n",
    "        'DestinationS3Uri': f\"s3://{data_capture_bucket}/{data_capture_prefix}\",\n",
    "        'CaptureOptions': [\n",
    "            {\n",
    "                'CaptureMode': 'Input'\n",
    "            },\n",
    "            {\n",
    "                'CaptureMode': 'Output'\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete an existing config with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws sagemaker delete-endpoint-config --endpoint-config-name $endpoint_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName=qa-model-from-registry-ep-1632465588\n",
      "arn:aws:sagemaker:us-east-1:093729152554:endpoint/qa-model-from-registry-ep-1632465588\n"
     ]
    }
   ],
   "source": [
    "pipeline_endpoint_name = \"qa-model-from-registry-ep-{}\".format(timestamp)\n",
    "print(\"EndpointName={}\".format(pipeline_endpoint_name))\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=pipeline_endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/qa-model-from-registry-ep-1632465588\">SageMaker REST Endpoint</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST Endpoint</a></b>'.format(\n",
    "            region, pipeline_endpoint_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 182 ms, sys: 19.2 ms, total: 202 ms\n",
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=pipeline_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List All Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Processing', 'StartTime': datetime.datetime(2021, 9, 22, 9, 27, 49, 711000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 16, 867000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-processing-jtj2dbws96'}}}\n",
      "pipelines-67ot3yw4s5kf-processing-jtj2dbws96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...22-08-43-33-756/input/code/preprocess.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/qa_raw.zip</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/processed/</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...22-08-43-33-756/input/code/preprocess.py     Input  DataSet   \n",
       "1              s3://sm-nlp-data/nlu/data/qa_raw.zip     Input  DataSet   \n",
       "2  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "3              s3://sm-nlp-data/nlu/data/processed/    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Train', 'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 256000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 630000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'CacheHitResult': {'SourcePipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/vqiccs4mntb5'}, 'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:training-job/pipelines-vqiccs4mntb5-train-jdmj5liqyt'}}}\n",
      "pipelines-vqiccs4mntb5-train-jdmj5liqyt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/processed/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76310...onaws.com/pytorch-training:1.8.1-gpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0              s3://sm-nlp-data/nlu/data/processed/     Input  DataSet   \n",
       "1  76310...onaws.com/pytorch-training:1.8.1-gpu-py3     Input    Image   \n",
       "2  s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'EvaluateModel', 'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 18, 202000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 55, 910000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-evaluatemodel-d7wij9sxb2'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...9-22-08-44-12-675/input/code/evaluate.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/processed/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...n-2021-09-22-08-42-52-197/output/metrics</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...9-22-08-44-12-675/input/code/evaluate.py     Input  DataSet   \n",
       "1              s3://sm-nlp-data/nlu/data/processed/     Input  DataSet   \n",
       "2  s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz     Input    Model   \n",
       "3  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "4  s3://...n-2021-09-22-08-42-52-197/output/metrics    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'IntentAndSlotCondition', 'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 375000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 675000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Condition': {'Outcome': 'True'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'QARegisterModel', 'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 298000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 187000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model-package/qamodelpackagegroup/7'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qamodelpackagegroup-7-Approved-1632311055-aws-...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76310...naws.com/pytorch-inference:1.8.1-gpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qamodelpackagegroup-7-PendingManualApproval-16...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QAModelPackageGroup-1631002331-aws-model-packa...</td>\n",
       "      <td>Output</td>\n",
       "      <td>ModelGroup</td>\n",
       "      <td>AssociatedWith</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name/Source Direction        Type  \\\n",
       "0  qamodelpackagegroup-7-Approved-1632311055-aws-...     Input    Approval   \n",
       "1   s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz     Input       Model   \n",
       "2   76310...naws.com/pytorch-inference:1.8.1-gpu-py3     Input       Image   \n",
       "3  qamodelpackagegroup-7-PendingManualApproval-16...     Input    Approval   \n",
       "4  QAModelPackageGroup-1631002331-aws-model-packa...    Output  ModelGroup   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo       action  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo       action  \n",
       "4   AssociatedWith      context  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'CreateQAModel', 'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 370000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 980000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model/pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Deployed Model\n",
    "\n",
    "CSVSerializer: [DOC](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html#sagemaker.serializers.CSVSerializer) </br>\n",
    "JSONDeserializer: [DOC](https://sagemaker.readthedocs.io/en/stable/api/inference/deserializers.html#sagemaker.deserializers.JSONDeserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.pytorch.model import PyTorchPredictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = PyTorchPredictor(\n",
    "    endpoint_name=pipeline_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch processed/psuedo/seq.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [['伊'],\n",
       "  ['坂'],\n",
       "  ['幸'],\n",
       "  ['太'],\n",
       "  ['郎'],\n",
       "  ['写'],\n",
       "  ['了'],\n",
       "  ['哪'],\n",
       "  ['些'],\n",
       "  ['书'],\n",
       "  [],\n",
       "  ['O'],\n",
       "  ['N'],\n",
       "  ['E'],\n",
       "  [],\n",
       "  ['P'],\n",
       "  ['I'],\n",
       "  ['E'],\n",
       "  ['C'],\n",
       "  ['E'],\n",
       "  ['総'],\n",
       "  ['集'],\n",
       "  ['编'],\n",
       "  [],\n",
       "  ['T'],\n",
       "  ['H'],\n",
       "  ['E'],\n",
       "  [],\n",
       "  ['F'],\n",
       "  ['I'],\n",
       "  ['R'],\n",
       "  ['S'],\n",
       "  ['T'],\n",
       "  [],\n",
       "  ['L'],\n",
       "  ['O'],\n",
       "  ['G'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['写'],\n",
       "  ['的'],\n",
       "  [],\n",
       "  ['高'],\n",
       "  ['效'],\n",
       "  ['管'],\n",
       "  ['理'],\n",
       "  ['W'],\n",
       "  ['i'],\n",
       "  ['n'],\n",
       "  ['d'],\n",
       "  ['o'],\n",
       "  ['w'],\n",
       "  ['s'],\n",
       "  ['网'],\n",
       "  ['络'],\n",
       "  ['/'],\n",
       "  ['W'],\n",
       "  ['i'],\n",
       "  ['n'],\n",
       "  ['3'],\n",
       "  ['2'],\n",
       "  [],\n",
       "  ['P'],\n",
       "  ['e'],\n",
       "  ['r'],\n",
       "  ['l'],\n",
       "  ['应'],\n",
       "  ['用'],\n",
       "  ['之'],\n",
       "  ['道'],\n",
       "  ['的'],\n",
       "  ['作'],\n",
       "  ['者'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  [],\n",
       "  ['D'],\n",
       "  ['a'],\n",
       "  ['v'],\n",
       "  ['e'],\n",
       "  ['写'],\n",
       "  ['了'],\n",
       "  ['什'],\n",
       "  ['么'],\n",
       "  ['书'],\n",
       "  [],\n",
       "  ['洪'],\n",
       "  ['荒'],\n",
       "  ['之'],\n",
       "  ['武'],\n",
       "  ['道'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['写'],\n",
       "  ['的'],\n",
       "  [],\n",
       "  ['玄'],\n",
       "  ['黄'],\n",
       "  ['真'],\n",
       "  ['人'],\n",
       "  ['写'],\n",
       "  ['了'],\n",
       "  ['哪'],\n",
       "  ['些'],\n",
       "  ['书'],\n",
       "  [],\n",
       "  ['风'],\n",
       "  ['景'],\n",
       "  ['景'],\n",
       "  ['观'],\n",
       "  ['工'],\n",
       "  ['程'],\n",
       "  ['体'],\n",
       "  ['系'],\n",
       "  ['化'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['的'],\n",
       "  ['作'],\n",
       "  ['品'],\n",
       "  [],\n",
       "  ['微'],\n",
       "  ['知'],\n",
       "  ['汇'],\n",
       "  ['：'],\n",
       "  ['万'],\n",
       "  ['物'],\n",
       "  ['简'],\n",
       "  ['史'],\n",
       "  ['的'],\n",
       "  ['作'],\n",
       "  ['者'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  [],\n",
       "  ['孽'],\n",
       "  ['阳'],\n",
       "  ['的'],\n",
       "  ['作'],\n",
       "  ['者'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  [],\n",
       "  ['茅'],\n",
       "  ['月'],\n",
       "  ['写'],\n",
       "  ['了'],\n",
       "  ['哪'],\n",
       "  ['些'],\n",
       "  ['书'],\n",
       "  [],\n",
       "  ['未'],\n",
       "  ['来'],\n",
       "  ['娱'],\n",
       "  ['乐'],\n",
       "  ['系'],\n",
       "  ['统'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['写'],\n",
       "  ['的'],\n",
       "  [],\n",
       "  ['小'],\n",
       "  ['僧'],\n",
       "  ['不'],\n",
       "  ['敲'],\n",
       "  ['木'],\n",
       "  ['鱼'],\n",
       "  ['有'],\n",
       "  ['什'],\n",
       "  ['么'],\n",
       "  ['著'],\n",
       "  ['作'],\n",
       "  [],\n",
       "  ['金'],\n",
       "  ['装'],\n",
       "  ['四'],\n",
       "  ['大'],\n",
       "  ['才'],\n",
       "  ['子'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['的'],\n",
       "  ['作'],\n",
       "  ['品'],\n",
       "  [],\n",
       "  ['罗'],\n",
       "  ['永'],\n",
       "  ['贤'],\n",
       "  ['导'],\n",
       "  ['演'],\n",
       "  ['了'],\n",
       "  ['哪'],\n",
       "  ['些'],\n",
       "  ['电'],\n",
       "  ['影'],\n",
       "  [],\n",
       "  ['为'],\n",
       "  ['了'],\n",
       "  ['你'],\n",
       "  ['我'],\n",
       "  ['愿'],\n",
       "  ['意'],\n",
       "  ['热'],\n",
       "  ['爱'],\n",
       "  ['整'],\n",
       "  ['个'],\n",
       "  ['世'],\n",
       "  ['界'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['的'],\n",
       "  ['作'],\n",
       "  ['品'],\n",
       "  [],\n",
       "  ['郭'],\n",
       "  ['虎'],\n",
       "  ['导'],\n",
       "  ['演'],\n",
       "  ['了'],\n",
       "  ['哪'],\n",
       "  ['些'],\n",
       "  ['电'],\n",
       "  ['影'],\n",
       "  [],\n",
       "  ['为'],\n",
       "  ['了'],\n",
       "  ['你'],\n",
       "  ['我'],\n",
       "  ['愿'],\n",
       "  ['意'],\n",
       "  ['热'],\n",
       "  ['爱'],\n",
       "  ['整'],\n",
       "  ['个'],\n",
       "  ['世'],\n",
       "  ['界'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['导'],\n",
       "  ['演'],\n",
       "  ['的'],\n",
       "  [],\n",
       "  ['灭'],\n",
       "  ['罪'],\n",
       "  ['师'],\n",
       "  ['的'],\n",
       "  ['导'],\n",
       "  ['演'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  [],\n",
       "  ['杨'],\n",
       "  ['苗'],\n",
       "  ['导'],\n",
       "  ['演'],\n",
       "  ['了'],\n",
       "  ['哪'],\n",
       "  ['些'],\n",
       "  ['电'],\n",
       "  ['视'],\n",
       "  ['剧'],\n",
       "  [],\n",
       "  ['灭'],\n",
       "  ['罪'],\n",
       "  ['师'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  ['导'],\n",
       "  ['演'],\n",
       "  ['的'],\n",
       "  [],\n",
       "  ['五'],\n",
       "  ['百'],\n",
       "  ['导'],\n",
       "  ['演'],\n",
       "  ['了'],\n",
       "  ['哪'],\n",
       "  ['些'],\n",
       "  ['电'],\n",
       "  ['视'],\n",
       "  ['剧'],\n",
       "  [],\n",
       "  ['穆'],\n",
       "  ['念'],\n",
       "  ['慈'],\n",
       "  ['的'],\n",
       "  ['丈'],\n",
       "  ['夫'],\n",
       "  ['是'],\n",
       "  ['谁'],\n",
       "  [],\n",
       "  ['杨'],\n",
       "  ['康'],\n",
       "  ['的'],\n",
       "  ['配'],\n",
       "  ['偶'],\n",
       "  ['是'],\n",
       "  ['谁']],\n",
       " 'intentions': ['ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_director',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_books',\n",
       "  'ask_author',\n",
       "  'ask_author',\n",
       "  'ask_husband',\n",
       "  'ask_author',\n",
       "  'ask_husband'],\n",
       " 'slot_labels': [['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_book'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['O'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['O'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  [],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['B_name'],\n",
       "  ['O'],\n",
       "  ['B_name'],\n",
       "  ['O']]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('processed/psuedo/seq.in', 'r') as f:\n",
    "    lines = f.read()\n",
    "    predicted = predictor.predict(lines)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Captured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "inference/qa-model-from-registry-ep-1632465588/AllTraffic/2021/09/24/07/27-23-527-83199569-6b92-41b3-8074-7ef23d55164a.jsonl\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}{}'.format(data_capture_prefix, pipeline_endpoint_name)\n",
    "result = s3.list_objects(Bucket=data_capture_bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inference/qa-model-from-registry-ep-1632465588/AllTraffic/2021/09/24/07/27-23-527-83199569-6b92-41b3-8074-7ef23d55164a.jsonl'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the S3Downloader utility to view and download the captured data in Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"5LyK5Z2C5bm45aSq6YOO5YaZ5LqG5ZOq5Lqb5LmmCgpPTkUgUElFQ0Xnt4/pm4bnvJYgVEhFIEZJUlNUIExPR+aYr+iwgeWGmeeahAoK6auY5pWI566h55CGV2luZG93c+e9kee7nC9XaW4zMiBQZXJs5bqU55So5LmL6YGT55qE5L2c6ICF5piv6LCBCgpEYXZl5YaZ5LqG5LuA5LmI5LmmCgrmtKrojZLkuYvmrabpgZPmmK/osIHlhpnnmoQKCueOhOm7hOecn+S6uuWGmeS6huWTquS6m+S5pgoK6aOO5pmv5pmv6KeC5bel56iL5L2T57O75YyW5piv6LCB55qE5L2c5ZOBCgrlvq7nn6XmsYfvvJrkuIfniannroDlj7LnmoTkvZzogIXmmK/osIEKCuWtvemYs+eahOS9nOiAheaYr+iwgQoK6IyF5pyI5YaZ5LqG5ZOq5Lqb5LmmCgrmnKrmnaXlqLHkuZDns7vnu5/mmK/osIHlhpnnmoQKCuWwj+WDp+S4jeaVsuacqOmxvOacieS7gOS5iOiRl+S9nAoK6YeR6KOF5Zub5aSn5omN5a2Q5piv6LCB55qE5L2c5ZOBCgrnvZfmsLjotKTlr7zmvJTkuoblk6rkupvnlLXlvbEKCuS4uuS6huS9oOaIkeaEv+aEj+eDreeIseaVtOS4quS4lueVjOaYr+iwgeeahOS9nOWTgQoK6YOt6JmO5a+85ryU5LqG5ZOq5Lqb55S15b2xCgrkuLrkuobkvaDmiJHmhL/mhI/ng63niLHmlbTkuKrkuJbnlYzmmK/osIHlr7zmvJTnmoQKCueBree9quW4iOeahOWvvOa8lOaYr+iwgQoK5p2o6IuX5a+85ryU5LqG5ZOq5Lqb55S16KeG5YmnCgrnga3nvarluIjmmK/osIHlr7zmvJTnmoQKCuS6lOeZvuWvvOa8lOS6huWTquS6m+eUteinhuWJpwoK56mG5b+15oWI55qE5LiI5aSr5piv6LCBCgrmnajlurfnmoTphY3lgbbmmK/osIE=\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"application/json\",\"mode\":\"OUTPUT\",\"data\":\"{"text": [["伊"], ["坂"], ["幸"], ["太"], ["郎"], ["写"], ["了"], ["哪"], ["些"], ["书"], [], [], ["O"], ["N"], ["E"], [], ["P"], ["I"], ["E"], ["C"], ["E"], ["総"], ["集"], ["编"], [], ["T"], ["H"], ["E"], [], ["F"], ["I"], ["R"], ["S"], ["T"], [], ["L"], ["O"], ["G"], ["是"], ["谁"], ["写"], ["的"], [], [], ["高"], ["效"], ["管"], ["理"], ["W"], ["i"], ["n"], ["d"], ["o"], ["w"], ["s"], ["网"], ["络"], ["/"], ["W"], ["i"], ["n"], ["3"], ["2"], [], ["P"], ["e"], ["r"], ["l"], ["应"], ["用"], ["之"], ["道"], ["的"], ["作"], ["者"], ["是"], ["谁"], [], [], ["D"], ["a"], ["v"], ["e"], ["写"], ["了"], ["什"], ["么"], ["书"], [], [], ["洪"], ["荒"], ["之"], ["武"], ["道"], ["是"], ["谁"], ["写"], ["的"], [], [], ["玄"], ["黄"], ["真"], ["人"], ["写"], ["了"], ["哪"], ["些"], ["书"], [], [], ["风"], ["景"], ["景"], ["观"], ["工"], ["程"], ["体"], ["系"], ["化"], ["是"], ["谁"], ["的"], ["作"], ["品"], [], [], ["微"], ["知"], ["汇"], ["："], ["万"], ["物"], ["简"], ["史"], ["的"], ["作"], ["者"], ["是"], ["谁"], [], [], ["孽"], ["阳"], ["的"], ["作"], ["者"], ["是"], ["谁"], [], [], ["茅"], ["月"], ["写"], ["了"], ["哪"], ["些"], ["书"], [], [], ["未"], ["来"], ["娱"], ["乐"], ["系"], ["统"], ["是"], ["谁"], ["写"], ["的"], [], [], ["小"], ["僧"], ["不"], ["敲"], ["木"], ["鱼"], ["有"], ["什"], ["么"], ["著"], ["作"], [], [], ["金"], ["装"], ["四"], ["大"], ["才"], ["子"], ["是"], ["谁"], ["的"], ["作"], ["品"], [], [], ["罗"], ["永"], ["贤"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["影"], [], [], ["为"], ["了"], ["你"], ["我"], ["愿"], ["意"], ["热"], ["爱"], ["整"], ["个"], ["世"], ["界"], ["是"], ["谁"], ["的"], ["作"], ["品"], [], [], ["郭"], ["虎"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["影"], [], [], ["为"], ["了"], ["你"], ["我"], ["愿"], ["意"], ["热"], ["爱"], ["整"], ["个"], ["世"], ["界"], ["是"], ["谁"], ["导"], ["演"], ["的"], [], [], ["灭"], ["罪"], ["师"], ["的"], ["导"], ["演"], ["是"], ["谁"], [], [], ["杨"], ["苗"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["视"], ["剧"], [], [], ["灭"], ["罪"], ["师"], ["是"], ["谁"], ["导"], ["演"], ["的"], [], [], ["五"], ["百"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["视"], ["剧"], [], [], ["穆"], ["念"], ["慈"], ["的"], ["丈"], ["夫"], ["是"], ["谁"], [], [], ["杨"], ["康"], ["的"], ["配"], ["偶"], ["是"], ["谁"]], "intentions": ["ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_books", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_husband"], "slot_labels": [["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["O"], ["B_name"], ["O"], ["O"], [], [], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"], ["B_name"], [], [], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["O"], ["B_name"], ["B_name"], [], [], ["O"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], [], [], ["O"], ["O"], ["B_name"], ["B_name"], ["O"], ["O"], ["B_name"], ["B_name"], [], [], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"]]}\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"55abd859-46f9-4210-ac45-669a3cec5dcd\",\"inferenceTime\":\"2021-09-24T07:27:23Z\"},\"eventVersion\":\"0\"}\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "traffic = S3Downloader.read_file(f\"s3://{data_capture_bucket}/{capture_files[0]}\")\n",
    "traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_input_data = json.loads(traffic)['captureData']['endpointInput']['data']\n",
    "endpoint_output_data = json.loads(traffic)['captureData']['endpointOutput']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode payload with base64 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'伊坂幸太郎写了哪些书\\n\\nONE PIECE総集编 THE FIRST LOG是谁写的\\n\\n高效管理Windows网络/Win32 Perl应用之道的作者是谁\\n\\nDave写了什么书\\n\\n洪荒之武道是谁写的\\n\\n玄黄真人写了哪些书\\n\\n风景景观工程体系化是谁的作品\\n\\n微知汇：万物简史的作者是谁\\n\\n孽阳的作者是谁\\n\\n茅月写了哪些书\\n\\n未来娱乐系统是谁写的\\n\\n小僧不敲木鱼有什么著作\\n\\n金装四大才子是谁的作品\\n\\n罗永贤导演了哪些电影\\n\\n为了你我愿意热爱整个世界是谁的作品\\n\\n郭虎导演了哪些电影\\n\\n为了你我愿意热爱整个世界是谁导演的\\n\\n灭罪师的导演是谁\\n\\n杨苗导演了哪些电视剧\\n\\n灭罪师是谁导演的\\n\\n五百导演了哪些电视剧\\n\\n穆念慈的丈夫是谁\\n\\n杨康的配偶是谁'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "base64.b64decode(endpoint_input_data).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"text\": [[\"伊\"], [\"坂\"], [\"幸\"], [\"太\"], [\"郎\"], [\"写\"], [\"了\"], [\"哪\"], [\"些\"], [\"书\"], [], [], [\"O\"], [\"N\"], [\"E\"], [], [\"P\"], [\"I\"], [\"E\"], [\"C\"], [\"E\"], [\"総\"], [\"集\"], [\"编\"], [], [\"T\"], [\"H\"], [\"E\"], [], [\"F\"], [\"I\"], [\"R\"], [\"S\"], [\"T\"], [], [\"L\"], [\"O\"], [\"G\"], [\"是\"], [\"谁\"], [\"写\"], [\"的\"], [], [], [\"高\"], [\"效\"], [\"管\"], [\"理\"], [\"W\"], [\"i\"], [\"n\"], [\"d\"], [\"o\"], [\"w\"], [\"s\"], [\"网\"], [\"络\"], [\"/\"], [\"W\"], [\"i\"], [\"n\"], [\"3\"], [\"2\"], [], [\"P\"], [\"e\"], [\"r\"], [\"l\"], [\"应\"], [\"用\"], [\"之\"], [\"道\"], [\"的\"], [\"作\"], [\"者\"], [\"是\"], [\"谁\"], [], [], [\"D\"], [\"a\"], [\"v\"], [\"e\"], [\"写\"], [\"了\"], [\"什\"], [\"么\"], [\"书\"], [], [], [\"洪\"], [\"荒\"], [\"之\"], [\"武\"], [\"道\"], [\"是\"], [\"谁\"], [\"写\"], [\"的\"], [], [], [\"玄\"], [\"黄\"], [\"真\"], [\"人\"], [\"写\"], [\"了\"], [\"哪\"], [\"些\"], [\"书\"], [], [], [\"风\"], [\"景\"], [\"景\"], [\"观\"], [\"工\"], [\"程\"], [\"体\"], [\"系\"], [\"化\"], [\"是\"], [\"谁\"], [\"的\"], [\"作\"], [\"品\"], [], [], [\"微\"], [\"知\"], [\"汇\"], [\"：\"], [\"万\"], [\"物\"], [\"简\"], [\"史\"], [\"的\"], [\"作\"], [\"者\"], [\"是\"], [\"谁\"], [], [], [\"孽\"], [\"阳\"], [\"的\"], [\"作\"], [\"者\"], [\"是\"], [\"谁\"], [], [], [\"茅\"], [\"月\"], [\"写\"], [\"了\"], [\"哪\"], [\"些\"], [\"书\"], [], [], [\"未\"], [\"来\"], [\"娱\"], [\"乐\"], [\"系\"], [\"统\"], [\"是\"], [\"谁\"], [\"写\"], [\"的\"], [], [], [\"小\"], [\"僧\"], [\"不\"], [\"敲\"], [\"木\"], [\"鱼\"], [\"有\"], [\"什\"], [\"么\"], [\"著\"], [\"作\"], [], [], [\"金\"], [\"装\"], [\"四\"], [\"大\"], [\"才\"], [\"子\"], [\"是\"], [\"谁\"], [\"的\"], [\"作\"], [\"品\"], [], [], [\"罗\"], [\"永\"], [\"贤\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"影\"], [], [], [\"为\"], [\"了\"], [\"你\"], [\"我\"], [\"愿\"], [\"意\"], [\"热\"], [\"爱\"], [\"整\"], [\"个\"], [\"世\"], [\"界\"], [\"是\"], [\"谁\"], [\"的\"], [\"作\"], [\"品\"], [], [], [\"郭\"], [\"虎\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"影\"], [], [], [\"为\"], [\"了\"], [\"你\"], [\"我\"], [\"愿\"], [\"意\"], [\"热\"], [\"爱\"], [\"整\"], [\"个\"], [\"世\"], [\"界\"], [\"是\"], [\"谁\"], [\"导\"], [\"演\"], [\"的\"], [], [], [\"灭\"], [\"罪\"], [\"师\"], [\"的\"], [\"导\"], [\"演\"], [\"是\"], [\"谁\"], [], [], [\"杨\"], [\"苗\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"视\"], [\"剧\"], [], [], [\"灭\"], [\"罪\"], [\"师\"], [\"是\"], [\"谁\"], [\"导\"], [\"演\"], [\"的\"], [], [], [\"五\"], [\"百\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"视\"], [\"剧\"], [], [], [\"穆\"], [\"念\"], [\"慈\"], [\"的\"], [\"丈\"], [\"夫\"], [\"是\"], [\"谁\"], [], [], [\"杨\"], [\"康\"], [\"的\"], [\"配\"], [\"偶\"], [\"是\"], [\"谁\"]], \"intentions\": [\"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_husband\"], \"slot_labels\": [[\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"O\"], [\"B_name\"], [\"O\"], [\"O\"], [], [], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [], [], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"]]}'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.b64decode(endpoint_output_data).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor SageMaker endpoints\n",
    "\n",
    "There are mainly data quality monitoring and model quality monitoring, in which:\n",
    "\n",
    "- data quality monitoring captures inference input, and compares data statistics like min, max with a baseline created from dataset [[Monitor Data Quality](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html)]\n",
    "- model quality monitoring monitors the performance of a model by comparing the predictions that the model makes with the actual ground truth labels that the model attempts to predict. [[Monitor Model Quality](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html)]\n",
    "\n",
    "Data quality is only applicapable for tabular data, therefore **not suitable** for this question understanding use case. Here we implement a quality monitoring for model quality.\n",
    "\n",
    "Reference:\n",
    "- AWS Doc: [Amazon SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)\n",
    "- SageMaker Doc: [Amazon SageMaker Model Monitor](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.html)\n",
    "- [AWS Workshop: Model Monitor](https://sagemaker-immersionday.workshop.aws/lab4/monitoring.html)\n",
    "- [Create a Model Quality Baseline](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-baseline.html)\n",
    "\n",
    "### Create a Model Quality Baseline\n",
    "\n",
    "1.  Create an instance of the ModelQualityMonitor class. \n",
    "\n",
    "Check SageMaker ModelQualityMonitor API: [Doc](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output_bucket = 'sm-nlp-data'\n",
    "baseline_job_name = \"QABaseLineJob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a baseline dataset in JSON with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/dev/seq.in') as f:\n",
    "    x_input = f.readlines()\n",
    "    x_input = [x.strip() for x in x_input]\n",
    "with open('processed/dev/label') as f:\n",
    "    y_output = f.readlines()\n",
    "    y_output = [y.strip() for y in y_output]\n",
    "with open('processed/dev/seq.out') as f:\n",
    "    seq_output = f.readlines()\n",
    "    seq_output = [seq.strip().split() for seq in seq_output]\n",
    "val_dataset = {\n",
    "    'seq_in': x_input,\n",
    "    'seq_out': seq_output,\n",
    "    'label': y_output\n",
    "}\n",
    "with open('val_dataset.json', 'w') as f:\n",
    "    json.dump(val_dataset, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now call the suggest_baseline method of the ModelQualityMonitor object to run a baseline job. We need a baseline dataset that contains both predictions and labels stored in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_job_name,\n",
    "    baseline_dataset='./val_dataset.json', # The S3 location of the validation dataset.\n",
    "    dataset_format=DatasetFormat.json(lines=False), # Whether the file should be read as a json object per line\n",
    "    output_s3_uri = f\"s3://{baseline_output_bucket}/{baseline_job_name}/\", # The S3 location to store the results.\n",
    "    problem_type='MulticlassClassification',\n",
    "    inference_attribute= \"label\", # The column in the dataset that contains predictions.\n",
    "    ground_truth_attribute= \"label\" # The column in the dataset that contains ground truth labels.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2021-09-24 09:08:07,412 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/qabaselinejob', 'ProcessingJobName': 'QABaseLineJob', 'Environment': {'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"json\": {\"lines\": false}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'label', 'output_path': '/opt/ml/processing/output', 'problem_type': 'MulticlassClassification', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-093729152554/model-monitor/baselining/QABaseLineJob/input/baseline_dataset_input', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sm-nlp-data/QABaseLineJob/', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.4xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,412 - __main__ - INFO - Current Environment:{'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"json\": {\"lines\": false}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'label', 'output_path': '/opt/ml/processing/output', 'problem_type': 'MulticlassClassification', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,412 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"json\": {\"lines\": false}}, \"output_path\": \"/opt/ml/processing/output\", \"analysis_type\": \"MODEL_QUALITY\", \"problem_type\": \"MulticlassClassification\", \"inference_attribute\": \"label\", \"probability_attribute\": null, \"ground_truth_attribute\": \"label\", \"probability_threshold_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\"}\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,412 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,413 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,470 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,470 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,470 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,480 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,481 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,481 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,885 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.245.124\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hado\u001b[0m\n",
      "\u001b[34mop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_282\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,891 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:07,895 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-e31c4b05-5bf3-468b-a6d2-ed1bae3a3dfe\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,280 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,294 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,295 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,298 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,303 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,303 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,303 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,303 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,338 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,350 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,350 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,354 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,357 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Sep 24 09:08:08\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,359 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,359 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,360 INFO util.GSet: 2.0% max memory 13.7 GB = 280.0 MB\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,360 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,450 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,454 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,455 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,455 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,484 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,484 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,484 INFO util.GSet: 1.0% max memory 13.7 GB = 140.0 MB\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,484 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,718 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,718 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,718 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,718 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,723 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,727 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,727 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,727 INFO util.GSet: 0.25% max memory 13.7 GB = 35.0 MB\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,727 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,735 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,735 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,735 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,738 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,738 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,739 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,739 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,739 INFO util.GSet: 0.029999999329447746% max memory 13.7 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,739 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,760 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1552458965-10.0.245.124-1632474488755\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,772 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,779 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,870 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,882 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,885 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.245.124\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:08,906 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:10,952 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:10,952 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:12,997 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:12,997 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:15,043 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:15,043 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:17,091 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:17,091 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:19,138 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:19,138 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:29,148 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  Main:28 - Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  Main:31 - Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  FileUtil:66 - Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SparkContext:54 - Running Spark version 2.3.1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SparkContext:54 - Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 41977.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SparkEnv:54 - Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SparkEnv:54 - Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-3660e3c3-9846-47ab-8a9a-2b15d51e6cdd\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  MemoryStore:54 - MemoryStore started with capacity 1458.6 MB\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:30 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  SparkContext:54 - Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.245.124:41977/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1632474511003\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  RMProxy:133 - Connecting to ResourceManager at /10.0.245.124:8032\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  Client:54 - Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  Configuration:2636 - resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  ResourceUtils:427 - Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (63622 MB per container)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  Client:54 - Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  Client:54 - Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:31 INFO  Client:54 - Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:32 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:34 INFO  Client:54 - Uploading resource file:/tmp/spark-2f53b33e-c567-4a49-b8df-a9f2a1d252f7/__spark_libs__4738344581207122666.zip -> hdfs://10.0.245.124/user/root/.sparkStaging/application_1632474494094_0001/__spark_libs__4738344581207122666.zip\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:34 INFO  Client:54 - Uploading resource file:/tmp/spark-2f53b33e-c567-4a49-b8df-a9f2a1d252f7/__spark_conf__1108021827462819016.zip -> hdfs://10.0.245.124/user/root/.sparkStaging/application_1632474494094_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  Client:54 - Submitting application application_1632474494094_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  YarnClientImpl:310 - Submitted application application_1632474494094_0001\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:35 INFO  SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1632474494094_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:36 INFO  Client:54 - Application report for application_1632474494094_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:36 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Fri Sep 24 09:08:35 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1632474515106\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1632474494094_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:37 INFO  Client:54 - Application report for application_1632474494094_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:38 INFO  Client:54 - Application report for application_1632474494094_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:39 INFO  Client:54 - Application report for application_1632474494094_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:39 INFO  YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1632474494094_0001), /proxy/application_1632474494094_0001\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:39 INFO  YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  Client:54 - Application report for application_1632474494094_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.245.124\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: 0\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1632474515106\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1632474494094_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  YarnClientSchedulerBackend:54 - Application application_1632474494094_0001 has started running.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46233.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  NettyBlockTransferService:54 - Server created on 10.0.245.124:46233\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.245.124, 46233, None)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.245.124:46233 with 1458.6 MB RAM, BlockManagerId(driver, 10.0.245.124, 46233, None)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.245.124, 46233, None)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.245.124, 46233, None)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:40 INFO  log:192 - Logging initialized @11088ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:41 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.245.124:56024) with ID 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:08:41 INFO  BlockManagerMasterEndpoint:54 - Registering block manager algo-1:41583 with 24.1 GB RAM, BlockManagerId(1, algo-1, 41583, None)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:01 INFO  YarnClientSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:01 WARN  SparkContext:66 - Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:01 INFO  DatasetReader:91 - Files to process:List(file:///opt/ml/processing/input/baseline_dataset_input/val_dataset.json)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:01 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.3.1/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:01 INFO  SharedState:54 - Warehouse path is 'file:/usr/spark-2.3.1/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:01 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  CodeGenerator:54 - Code generated in 154.780025 ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 429.4 KB, free 1458.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.245.124:46233 (size: 38.3 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  SparkContext:54 - Created broadcast 0 from json at DatasetReader.scala:52\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  SparkContext:54 - Starting job: json at DatasetReader.scala:52\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  DAGScheduler:54 - Got job 0 (json at DatasetReader.scala:52) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (json at DatasetReader.scala:52)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  DAGScheduler:54 - Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  DAGScheduler:54 - Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at DatasetReader.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.245.124:46233 (size: 5.1 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at DatasetReader.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  YarnScheduler:54 - Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:02 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8347 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:03 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on algo-1:41583 (size: 5.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on algo-1:41583 (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 1520 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  YarnScheduler:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - ResultStage 0 (json at DatasetReader.scala:52) finished in 1.568 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Job 0 finished: json at DatasetReader.scala:52, took 1.605723 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  FileSourceStrategy:54 - Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  CodeGenerator:54 - Code generated in 17.358881 ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  CodeGenerator:54 - Code generated in 11.952182 ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 429.4 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.245.124:46233 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  SparkContext:54 - Created broadcast 2 from count at MulticlassClassificationAnalyzer.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  SparkContext:54 - Starting job: count at MulticlassClassificationAnalyzer.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Registering RDD 6 (count at MulticlassClassificationAnalyzer.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Got job 1 (count at MulticlassClassificationAnalyzer.scala:53) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (count at MulticlassClassificationAnalyzer.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at MulticlassClassificationAnalyzer.scala:53), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 11.7 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 10.0.245.124:46233 (size: 6.1 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at MulticlassClassificationAnalyzer.scala:53) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  YarnScheduler:54 - Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on algo-1:41583 (size: 6.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on algo-1:41583 (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 158 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  YarnScheduler:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - ShuffleMapStage 1 (count at MulticlassClassificationAnalyzer.scala:53) finished in 0.173 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - waiting: Set(ResultStage 2)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Submitting ResultStage 2 (MapPartitionsRDD[9] at count at MulticlassClassificationAnalyzer.scala:53), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 10.0.245.124:46233 (size: 3.8 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at MulticlassClassificationAnalyzer.scala:53) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  YarnScheduler:54 - Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on algo-1:41583 (size: 3.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  YarnScheduler:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - ResultStage 2 (count at MulticlassClassificationAnalyzer.scala:53) finished in 0.101 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:04 INFO  DAGScheduler:54 - Job 1 finished: count at MulticlassClassificationAnalyzer.scala:53, took 0.291459 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  FileSourceStrategy:54 - Output Data Schema: struct<label: array<string>>\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  CodeGenerator:54 - Code generated in 22.007398 ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 429.4 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on 10.0.245.124:46233 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  SparkContext:54 - Created broadcast 5 from rdd at ModelQualityAnalyzer.scala:296\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  SparkContext:54 - Starting job: collectAsMap at ModelQualityAnalyzer.scala:296\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  DAGScheduler:54 - Registering RDD 14 (rdd at ModelQualityAnalyzer.scala:296)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  DAGScheduler:54 - Got job 2 (collectAsMap at ModelQualityAnalyzer.scala:296) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (collectAsMap at ModelQualityAnalyzer.scala:296)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at rdd at ModelQualityAnalyzer.scala:296), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 19.6 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.2 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 10.0.245.124:46233 (size: 9.2 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at rdd at ModelQualityAnalyzer.scala:296) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  YarnScheduler:54 - Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:05 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on algo-1:41583 (size: 9.2 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on algo-1:41583 (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 926 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - ShuffleMapStage 3 (rdd at ModelQualityAnalyzer.scala:296) finished in 0.957 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - waiting: Set(ResultStage 4)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting ResultStage 4 (ShuffledRDD[15] at reduceByKey at ModelQualityAnalyzer.scala:296), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1965.0 B, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 10.0.245.124:46233 (size: 1965.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (ShuffledRDD[15] at reduceByKey at ModelQualityAnalyzer.scala:296) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on algo-1:41583 (size: 1965.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 1 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 42 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - ResultStage 4 (collectAsMap at ModelQualityAnalyzer.scala:296) finished in 0.054 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Job 2 finished: collectAsMap at ModelQualityAnalyzer.scala:296, took 1.020267 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<label: array<string>>\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  CodeGenerator:54 - Code generated in 14.859555 ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 429.4 KB, free 1456.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1456.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on 10.0.245.124:46233 (size: 38.3 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 8 from rdd at MulticlassClassificationAnalyzer.scala:149\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<label: array<string>>\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  CodeGenerator:54 - Code generated in 10.487343 ms\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 429.4 KB, free 1456.3 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1456.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on 10.0.245.124:46233 (size: 38.3 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 9 from rdd at MulticlassClassificationAnalyzer.scala:153\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:48\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Registering RDD 26 (map at MulticlassMetrics.scala:45)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Got job 3 (collectAsMap at MulticlassMetrics.scala:48) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Final stage: ResultStage 6 (collectAsMap at MulticlassMetrics.scala:48)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 5 (MapPartitionsRDD[26] at map at MulticlassMetrics.scala:45), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 18.1 KB, free 1456.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.1 KB, free 1456.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on 10.0.245.124:46233 (size: 9.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[26] at map at MulticlassMetrics.scala:45) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on algo-1:41583 (size: 9.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on algo-1:41583 (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 392 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - ShuffleMapStage 5 (map at MulticlassMetrics.scala:45) finished in 0.412 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - waiting: Set(ResultStage 6)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting ResultStage 6 (ShuffledRDD[27] at reduceByKey at MulticlassMetrics.scala:47), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 1456.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1456.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on 10.0.245.124:46233 (size: 1975.0 B, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 11 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 6 (ShuffledRDD[27] at reduceByKey at MulticlassMetrics.scala:47) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on algo-1:41583 (size: 1975.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 2 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - ResultStage 6 (collectAsMap at MulticlassMetrics.scala:48) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Job 3 finished: collectAsMap at MulticlassMetrics.scala:48, took 0.453388 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Starting job: countByValue at MulticlassMetrics.scala:42\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Registering RDD 30 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Got job 4 (countByValue at MulticlassMetrics.scala:42) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 7 (MapPartitionsRDD[30] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  ContextCleaner:54 - Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 18.7 KB, free 1456.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 9.2 KB, free 1456.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on 10.0.245.124:46233 (size: 9.2 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 12 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on 10.0.245.124:46233 in memory (size: 6.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[30] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on algo-1:41583 (size: 9.2 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on algo-1:41583 in memory (size: 6.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 10.0.245.124:46233 in memory (size: 3.8 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on algo-1:41583 in memory (size: 3.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on 10.0.245.124:46233 in memory (size: 9.2 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on algo-1:41583 in memory (size: 9.2 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 10.0.245.124:46233 in memory (size: 5.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on algo-1:41583 in memory (size: 5.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on 10.0.245.124:46233 in memory (size: 1975.0 B, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on algo-1:41583 in memory (size: 1975.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on 10.0.245.124:46233 in memory (size: 38.3 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on algo-1:41583 in memory (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on 10.0.245.124:46233 in memory (size: 1965.0 B, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on algo-1:41583 in memory (size: 1965.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on 10.0.245.124:46233 in memory (size: 9.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on algo-1:41583 in memory (size: 9.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 43 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  ContextCleaner:54 - Cleaned shuffle 2\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - ShuffleMapStage 7 (countByValue at MulticlassMetrics.scala:42) finished in 0.052 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on 10.0.245.124:46233 in memory (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - waiting: Set(ResultStage 8)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting ResultStage 8 (ShuffledRDD[31] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on algo-1:41583 in memory (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on 10.0.245.124:46233 in memory (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on algo-1:41583 in memory (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 1947.0 B, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on 10.0.245.124:46233 (size: 1947.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  ContextCleaner:54 - Cleaned shuffle 0\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 8 (ShuffledRDD[31] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Adding task set 8.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on algo-1:41583 (size: 1947.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 3 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - ResultStage 8 (countByValue at MulticlassMetrics.scala:42) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Job 4 finished: countByValue at MulticlassMetrics.scala:42, took 0.094969 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Registering RDD 32 (map at MulticlassMetrics.scala:50)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Got job 5 (collectAsMap at MulticlassMetrics.scala:53) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (collectAsMap at MulticlassMetrics.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 9 (MapPartitionsRDD[32] at map at MulticlassMetrics.scala:50), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 18.1 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.1 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on 10.0.245.124:46233 (size: 9.1 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  SparkContext:54 - Created broadcast 14 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[32] at map at MulticlassMetrics.scala:50) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  YarnScheduler:54 - Adding task set 9.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:06 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on algo-1:41583 (size: 9.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ShuffleMapStage 9 (map at MulticlassMetrics.scala:50) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ResultStage 10 (ShuffledRDD[33] at reduceByKey at MulticlassMetrics.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 3.2 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 1974.0 B, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on 10.0.245.124:46233 (size: 1974.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 15 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[33] at reduceByKey at MulticlassMetrics.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on algo-1:41583 (size: 1974.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 4 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ResultStage 10 (collectAsMap at MulticlassMetrics.scala:53) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Job 5 finished: collectAsMap at MulticlassMetrics.scala:53, took 0.064877 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:48\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Registering RDD 34 (map at MulticlassMetrics.scala:45)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Got job 6 (collectAsMap at MulticlassMetrics.scala:48) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Final stage: ResultStage 12 (collectAsMap at MulticlassMetrics.scala:48)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 11 (MapPartitionsRDD[34] at map at MulticlassMetrics.scala:45), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 17.2 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on 10.0.245.124:46233 (size: 8.9 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 16 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[34] at map at MulticlassMetrics.scala:45) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 11.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 11, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on algo-1:41583 (size: 8.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on algo-1:41583 (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 11) in 51 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ShuffleMapStage 11 (map at MulticlassMetrics.scala:45) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - waiting: Set(ResultStage 12)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ResultStage 12 (ShuffledRDD[35] at reduceByKey at MulticlassMetrics.scala:47), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 3.2 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 1973.0 B, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on 10.0.245.124:46233 (size: 1973.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 17 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 12 (ShuffledRDD[35] at reduceByKey at MulticlassMetrics.scala:47) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 12.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 12, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on algo-1:41583 (size: 1973.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 5 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 12) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ResultStage 12 (collectAsMap at MulticlassMetrics.scala:48) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Job 6 finished: collectAsMap at MulticlassMetrics.scala:48, took 0.094074 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Starting job: countByValue at MulticlassMetrics.scala:42\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Registering RDD 38 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Got job 7 (countByValue at MulticlassMetrics.scala:42) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 13 (MapPartitionsRDD[38] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 17.7 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 9.1 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on 10.0.245.124:46233 (size: 9.1 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 18 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[38] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 13.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 13, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on algo-1:41583 (size: 9.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 13) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ShuffleMapStage 13 (countByValue at MulticlassMetrics.scala:42) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - waiting: Set(ResultStage 14)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ResultStage 14 (ShuffledRDD[39] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 3.2 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 1947.0 B, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on 10.0.245.124:46233 (size: 1947.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 19 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 14 (ShuffledRDD[39] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 14.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 14, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on algo-1:41583 (size: 1947.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 6 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 14) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ResultStage 14 (countByValue at MulticlassMetrics.scala:42) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Job 7 finished: countByValue at MulticlassMetrics.scala:42, took 0.066648 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Registering RDD 40 (map at MulticlassMetrics.scala:50)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Got job 8 (collectAsMap at MulticlassMetrics.scala:53) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Final stage: ResultStage 16 (collectAsMap at MulticlassMetrics.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 15 (MapPartitionsRDD[40] at map at MulticlassMetrics.scala:50), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 17.2 KB, free 1457.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.9 KB, free 1457.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on 10.0.245.124:46233 (size: 8.9 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 20 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[40] at map at MulticlassMetrics.scala:50) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 15.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 15, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on algo-1:41583 (size: 8.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 15) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ShuffleMapStage 15 (map at MulticlassMetrics.scala:50) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - waiting: Set(ResultStage 16)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ResultStage 16 (ShuffledRDD[41] at reduceByKey at MulticlassMetrics.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_21 stored as values in memory (estimated size 3.2 KB, free 1457.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_21_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1457.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on 10.0.245.124:46233 (size: 1975.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 21 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 16 (ShuffledRDD[41] at reduceByKey at MulticlassMetrics.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 16.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 16, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on algo-1:41583 (size: 1975.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 7 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 16) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ResultStage 16 (collectAsMap at MulticlassMetrics.scala:53) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Job 8 finished: collectAsMap at MulticlassMetrics.scala:53, took 0.070965 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  ModelQualityAnalyzer$:198 - Not enough samples to compute standard deviation, skipping\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  FileSourceStrategy:54 - Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_22 stored as values in memory (estimated size 429.4 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_22_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on 10.0.245.124:46233 (size: 38.3 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 22 from count at ModelQualityAnalyzer.scala:137\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Starting job: count at ModelQualityAnalyzer.scala:137\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Registering RDD 44 (count at ModelQualityAnalyzer.scala:137)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Got job 9 (count at ModelQualityAnalyzer.scala:137) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Final stage: ResultStage 18 (count at ModelQualityAnalyzer.scala:137)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 17 (MapPartitionsRDD[44] at count at ModelQualityAnalyzer.scala:137), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_23 stored as values in memory (estimated size 11.7 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on 10.0.245.124:46233 (size: 6.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 23 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[44] at count at ModelQualityAnalyzer.scala:137) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 17.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 17.0 (TID 17, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on algo-1:41583 (size: 6.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on algo-1:41583 (size: 38.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 17.0 (TID 17) in 43 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ShuffleMapStage 17 (count at ModelQualityAnalyzer.scala:137) finished in 0.050 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - waiting: Set(ResultStage 18)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting ResultStage 18 (MapPartitionsRDD[47] at count at ModelQualityAnalyzer.scala:137), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_24 stored as values in memory (estimated size 7.4 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on 10.0.245.124:46233 (size: 3.8 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Created broadcast 24 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[47] at count at ModelQualityAnalyzer.scala:137) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Adding task set 18.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 18.0 (TID 18, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on algo-1:41583 (size: 3.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 8 to 10.0.245.124:56024\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 18.0 (TID 18) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnScheduler:54 - Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - ResultStage 18 (count at ModelQualityAnalyzer.scala:137) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  DAGScheduler:54 - Job 9 finished: count at ModelQualityAnalyzer.scala:137, took 0.082209 s\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  CWMetricsWriterImpl$:24 - Cloudwatch metric publishing is disabled, not publishing metrics\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  FileUtil:29 - Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  FileUtil:29 - Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnClientSchedulerBackend:54 - Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnClientSchedulerBackend:54 - Shutting down all executors\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  YarnClientSchedulerBackend:54 - Stopped\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  MemoryStore:54 - MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManager:54 - BlockManager stopped\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  SparkContext:54 - Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  Main:65 - Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  Main:141 - Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  ShutdownHookManager:54 - Shutdown hook called\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-2f53b33e-c567-4a49-b8df-a9f2a1d252f7\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-09f7dcb0-9e75-4cf8-8b76-6783b06fee3e\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07,907 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2021-09-24 09:09:07,908 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "job.wait(logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule Model Quality Monitoring Jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Ground Truth Labels and Merge Them With Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
