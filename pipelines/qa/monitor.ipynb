{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment and Monitor\n",
    "\n",
    "This notebook deploy a model trained in pipeline, and monitor it.\n",
    "\n",
    "## Update Model Package Approval Status\n",
    "\n",
    "We can approve the model using the SageMaker Studio UI or programmatically as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'qa-pipeline-16323001711632300171'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive Model From Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n",
      "[{'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/67ot3yw4s5kf',\n",
      "  'PipelineExecutionDisplayName': 'nidome',\n",
      "  'PipelineExecutionStatus': 'Succeeded',\n",
      "  'StartTime': datetime.datetime(2021, 9, 22, 9, 27, 48, 609000, tzinfo=tzlocal())},\n",
      " {'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/vqiccs4mntb5',\n",
      "  'PipelineExecutionDisplayName': 'execution-1632300253350',\n",
      "  'PipelineExecutionStatus': 'Succeeded',\n",
      "  'StartTime': datetime.datetime(2021, 9, 22, 8, 44, 13, 274000, tzinfo=tzlocal())}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status == \"Executing\":\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "        pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "    #        print('Executions for our pipeline...')\n",
    "    #        print(pipeline_execution_status)\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(executions_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/67ot3yw4s5kf\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_arn = executions_response[0][\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PipelineExecutionSteps': [{'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 980000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model/pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 370000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'CreateQAModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 187000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model-package/qamodelpackagegroup/7'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 298000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'QARegisterModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 675000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'Condition': {'Outcome': 'True'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 375000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'IntentAndSlotCondition',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 55, 910000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-evaluatemodel-d7wij9sxb2'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 18, 202000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'EvaluateModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'CacheHitResult': {'SourcePipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/vqiccs4mntb5'},\n",
      "                             'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 630000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:training-job/pipelines-vqiccs4mntb5-train-jdmj5liqyt'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 256000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'Train',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 16, 867000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-processing-jtj2dbws96'}},\n",
      "                             'StartTime': datetime.datetime(2021, 9, 22, 9, 27, 49, 711000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'Processing',\n",
      "                             'StepStatus': 'Succeeded'}],\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '1642',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 24 Sep 2021 06:21:52 GMT',\n",
      "                                      'x-amzn-requestid': 'dd666965-5d55-4e0c-b58c-2a2804b7524d'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'dd666965-5d55-4e0c-b58c-2a2804b7524d',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)\n",
    "\n",
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Registered Model and Update Model Approval Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:model-package/qamodelpackagegroup/7\n"
     ]
    }
   ],
   "source": [
    "for execution_step in steps[\"PipelineExecutionSteps\"]:\n",
    "    if execution_step[\"StepName\"] == \"QARegisterModel\":\n",
    "        model_package_arn = execution_step[\"Metadata\"][\"RegisterModel\"][\"Arn\"]\n",
    "        break\n",
    "print(model_package_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_response = sm.update_model_package(\n",
    "    ModelPackageArn=model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",  # Other options are Rejected and PendingManualApproval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:model/pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg\n",
      "created_model_name: pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg\n"
     ]
    }
   ],
   "source": [
    "for execution_step in steps[\"PipelineExecutionSteps\"]:\n",
    "    if execution_step[\"StepName\"] == \"CreateQAModel\":\n",
    "        model_arn = execution_step[\"Metadata\"][\"Model\"][\"Arn\"]\n",
    "        break\n",
    "print(model_arn)\n",
    "\n",
    "created_model_name = model_arn.split(\"/\")[-1]\n",
    "print('created_model_name:', created_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Endpoint from Model Registry and Configure It to Capture Requests\n",
    "\n",
    "### Create model from registry\n",
    "\n",
    "More details here: https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-deploy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from registry name : qa-model-from-registry-1632465588\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "model_from_registry_name = \"qa-model-from-registry-{}\".format(timestamp)\n",
    "print(\"Model from registry name : {}\".format(model_from_registry_name))\n",
    "\n",
    "model_registry_package_container = {\n",
    "    \"ModelPackageName\": model_package_arn,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:093729152554:model/qa-model-from-registry-1632465588',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '95',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 24 Sep 2021 06:39:49 GMT',\n",
      "                                      'x-amzn-requestid': 'b922cc61-ab4e-4cd4-b067-2212e5acfa89'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'b922cc61-ab4e-4cd4-b067-2212e5acfa89',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "create_model_from_registry_respose = sm.create_model(\n",
    "    ModelName=model_from_registry_name, ExecutionRoleArn=role, PrimaryContainer=model_registry_package_container\n",
    ")\n",
    "pprint(create_model_from_registry_respose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-east-1:093729152554:model/qa-model-from-registry-1632465588'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_from_registry_arn = create_model_from_registry_respose[\"ModelArn\"]\n",
    "model_from_registry_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Endpoint to Capture Data from Requests and Responses\n",
    "\n",
    "Check API for [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_bucket = 'sm-nlp-data'\n",
    "data_capture_prefix = 'inference/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an endpoint configuration that Amazon SageMaker hosting services uses to deploy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa-model-from-registry-epc-1632465588\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"qa-model-from-registry-epc-{}\".format(timestamp)\n",
    "print(endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": created_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    "    DataCaptureConfig={\n",
    "        'EnableCapture': True,\n",
    "        'InitialSamplingPercentage': 100,\n",
    "        'DestinationS3Uri': f\"s3://{data_capture_bucket}/{data_capture_prefix}\",\n",
    "        'CaptureOptions': [\n",
    "            {\n",
    "                'CaptureMode': 'Input'\n",
    "            },\n",
    "            {\n",
    "                'CaptureMode': 'Output'\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete an existing config with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws sagemaker delete-endpoint-config --endpoint-config-name $endpoint_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName=qa-model-from-registry-ep-1632465588\n",
      "arn:aws:sagemaker:us-east-1:093729152554:endpoint/qa-model-from-registry-ep-1632465588\n"
     ]
    }
   ],
   "source": [
    "pipeline_endpoint_name = \"qa-model-from-registry-ep-{}\".format(timestamp)\n",
    "print(\"EndpointName={}\".format(pipeline_endpoint_name))\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=pipeline_endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/qa-model-from-registry-ep-1632465588\">SageMaker REST Endpoint</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST Endpoint</a></b>'.format(\n",
    "            region, pipeline_endpoint_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 182 ms, sys: 19.2 ms, total: 202 ms\n",
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=pipeline_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List All Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Processing', 'StartTime': datetime.datetime(2021, 9, 22, 9, 27, 49, 711000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 16, 867000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-processing-jtj2dbws96'}}}\n",
      "pipelines-67ot3yw4s5kf-processing-jtj2dbws96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...22-08-43-33-756/input/code/preprocess.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/qa_raw.zip</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/processed/</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...22-08-43-33-756/input/code/preprocess.py     Input  DataSet   \n",
       "1              s3://sm-nlp-data/nlu/data/qa_raw.zip     Input  DataSet   \n",
       "2  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "3              s3://sm-nlp-data/nlu/data/processed/    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Train', 'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 256000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 32, 17, 630000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'CacheHitResult': {'SourcePipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/qa-pipeline-16323001711632300171/execution/vqiccs4mntb5'}, 'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:training-job/pipelines-vqiccs4mntb5-train-jdmj5liqyt'}}}\n",
      "pipelines-vqiccs4mntb5-train-jdmj5liqyt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/processed/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76310...onaws.com/pytorch-training:1.8.1-gpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0              s3://sm-nlp-data/nlu/data/processed/     Input  DataSet   \n",
       "1  76310...onaws.com/pytorch-training:1.8.1-gpu-py3     Input    Image   \n",
       "2  s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'EvaluateModel', 'StartTime': datetime.datetime(2021, 9, 22, 9, 32, 18, 202000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 55, 910000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/pipelines-67ot3yw4s5kf-evaluatemodel-d7wij9sxb2'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...9-22-08-44-12-675/input/code/evaluate.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://sm-nlp-data/nlu/data/processed/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...n-2021-09-22-08-42-52-197/output/metrics</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...9-22-08-44-12-675/input/code/evaluate.py     Input  DataSet   \n",
       "1              s3://sm-nlp-data/nlu/data/processed/     Input  DataSet   \n",
       "2  s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz     Input    Model   \n",
       "3  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "4  s3://...n-2021-09-22-08-42-52-197/output/metrics    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'IntentAndSlotCondition', 'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 375000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 56, 675000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Condition': {'Outcome': 'True'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'QARegisterModel', 'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 298000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 187000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model-package/qamodelpackagegroup/7'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qamodelpackagegroup-7-Approved-1632311055-aws-...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76310...naws.com/pytorch-inference:1.8.1-gpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qamodelpackagegroup-7-PendingManualApproval-16...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QAModelPackageGroup-1631002331-aws-model-packa...</td>\n",
       "      <td>Output</td>\n",
       "      <td>ModelGroup</td>\n",
       "      <td>AssociatedWith</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name/Source Direction        Type  \\\n",
       "0  qamodelpackagegroup-7-Approved-1632311055-aws-...     Input    Approval   \n",
       "1   s3://...tb5-Train-JDmj5LiQyt/output/model.tar.gz     Input       Model   \n",
       "2   76310...naws.com/pytorch-inference:1.8.1-gpu-py3     Input       Image   \n",
       "3  qamodelpackagegroup-7-PendingManualApproval-16...     Input    Approval   \n",
       "4  QAModelPackageGroup-1631002331-aws-model-packa...    Output  ModelGroup   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo       action  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo       action  \n",
       "4   AssociatedWith      context  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'CreateQAModel', 'StartTime': datetime.datetime(2021, 9, 22, 9, 37, 57, 370000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 9, 22, 9, 37, 58, 980000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:093729152554:model/pipelines-67ot3yw4s5kf-createqamodel-en4hwj8lbg'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Deployed Model\n",
    "\n",
    "CSVSerializer: [DOC](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html#sagemaker.serializers.CSVSerializer) </br>\n",
    "JSONDeserializer: [DOC](https://sagemaker.readthedocs.io/en/stable/api/inference/deserializers.html#sagemaker.deserializers.JSONDeserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_name='qa-model-from-registry-ep-1632465588'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.pytorch.model import PyTorchPredictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = PyTorchPredictor(\n",
    "    endpoint_name=pipeline_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try predict on some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/psuedo/seq.in', 'r') as f:\n",
    "    lines = f.read()\n",
    "    predicted = predictor.predict(lines)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be too long and go out of wait limitation if we predict all the data all at once, so we split them into little chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [09:56<00:00, 33.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "chunk_size = 20\n",
    "predicted_cls = []\n",
    "with open('processed/test/seq.in') as f:\n",
    "    lines = f.readlines()\n",
    "    chunks = [lines[i: i+chunk_size] for i in range(0, len(lines), chunk_size)]\n",
    "    for chunk in tqdm(chunks):\n",
    "        predicted = predictor.predict(chunk)\n",
    "        predicted_cls += predicted['intentions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Captured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "inference/qa-model-from-registry-ep-1632465588/AllTraffic/2021/09/24/07/27-23-527-83199569-6b92-41b3-8074-7ef23d55164a.jsonl\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}{}'.format(data_capture_prefix, pipeline_endpoint_name)\n",
    "result = s3.list_objects(Bucket=data_capture_bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inference/qa-model-from-registry-ep-1632465588/AllTraffic/2021/09/24/07/27-23-527-83199569-6b92-41b3-8074-7ef23d55164a.jsonl'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the S3Downloader utility to view and download the captured data in Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"5LyK5Z2C5bm45aSq6YOO5YaZ5LqG5ZOq5Lqb5LmmCgpPTkUgUElFQ0Xnt4/pm4bnvJYgVEhFIEZJUlNUIExPR+aYr+iwgeWGmeeahAoK6auY5pWI566h55CGV2luZG93c+e9kee7nC9XaW4zMiBQZXJs5bqU55So5LmL6YGT55qE5L2c6ICF5piv6LCBCgpEYXZl5YaZ5LqG5LuA5LmI5LmmCgrmtKrojZLkuYvmrabpgZPmmK/osIHlhpnnmoQKCueOhOm7hOecn+S6uuWGmeS6huWTquS6m+S5pgoK6aOO5pmv5pmv6KeC5bel56iL5L2T57O75YyW5piv6LCB55qE5L2c5ZOBCgrlvq7nn6XmsYfvvJrkuIfniannroDlj7LnmoTkvZzogIXmmK/osIEKCuWtvemYs+eahOS9nOiAheaYr+iwgQoK6IyF5pyI5YaZ5LqG5ZOq5Lqb5LmmCgrmnKrmnaXlqLHkuZDns7vnu5/mmK/osIHlhpnnmoQKCuWwj+WDp+S4jeaVsuacqOmxvOacieS7gOS5iOiRl+S9nAoK6YeR6KOF5Zub5aSn5omN5a2Q5piv6LCB55qE5L2c5ZOBCgrnvZfmsLjotKTlr7zmvJTkuoblk6rkupvnlLXlvbEKCuS4uuS6huS9oOaIkeaEv+aEj+eDreeIseaVtOS4quS4lueVjOaYr+iwgeeahOS9nOWTgQoK6YOt6JmO5a+85ryU5LqG5ZOq5Lqb55S15b2xCgrkuLrkuobkvaDmiJHmhL/mhI/ng63niLHmlbTkuKrkuJbnlYzmmK/osIHlr7zmvJTnmoQKCueBree9quW4iOeahOWvvOa8lOaYr+iwgQoK5p2o6IuX5a+85ryU5LqG5ZOq5Lqb55S16KeG5YmnCgrnga3nvarluIjmmK/osIHlr7zmvJTnmoQKCuS6lOeZvuWvvOa8lOS6huWTquS6m+eUteinhuWJpwoK56mG5b+15oWI55qE5LiI5aSr5piv6LCBCgrmnajlurfnmoTphY3lgbbmmK/osIE=\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"application/json\",\"mode\":\"OUTPUT\",\"data\":\"{"text": [["伊"], ["坂"], ["幸"], ["太"], ["郎"], ["写"], ["了"], ["哪"], ["些"], ["书"], [], [], ["O"], ["N"], ["E"], [], ["P"], ["I"], ["E"], ["C"], ["E"], ["総"], ["集"], ["编"], [], ["T"], ["H"], ["E"], [], ["F"], ["I"], ["R"], ["S"], ["T"], [], ["L"], ["O"], ["G"], ["是"], ["谁"], ["写"], ["的"], [], [], ["高"], ["效"], ["管"], ["理"], ["W"], ["i"], ["n"], ["d"], ["o"], ["w"], ["s"], ["网"], ["络"], ["/"], ["W"], ["i"], ["n"], ["3"], ["2"], [], ["P"], ["e"], ["r"], ["l"], ["应"], ["用"], ["之"], ["道"], ["的"], ["作"], ["者"], ["是"], ["谁"], [], [], ["D"], ["a"], ["v"], ["e"], ["写"], ["了"], ["什"], ["么"], ["书"], [], [], ["洪"], ["荒"], ["之"], ["武"], ["道"], ["是"], ["谁"], ["写"], ["的"], [], [], ["玄"], ["黄"], ["真"], ["人"], ["写"], ["了"], ["哪"], ["些"], ["书"], [], [], ["风"], ["景"], ["景"], ["观"], ["工"], ["程"], ["体"], ["系"], ["化"], ["是"], ["谁"], ["的"], ["作"], ["品"], [], [], ["微"], ["知"], ["汇"], ["："], ["万"], ["物"], ["简"], ["史"], ["的"], ["作"], ["者"], ["是"], ["谁"], [], [], ["孽"], ["阳"], ["的"], ["作"], ["者"], ["是"], ["谁"], [], [], ["茅"], ["月"], ["写"], ["了"], ["哪"], ["些"], ["书"], [], [], ["未"], ["来"], ["娱"], ["乐"], ["系"], ["统"], ["是"], ["谁"], ["写"], ["的"], [], [], ["小"], ["僧"], ["不"], ["敲"], ["木"], ["鱼"], ["有"], ["什"], ["么"], ["著"], ["作"], [], [], ["金"], ["装"], ["四"], ["大"], ["才"], ["子"], ["是"], ["谁"], ["的"], ["作"], ["品"], [], [], ["罗"], ["永"], ["贤"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["影"], [], [], ["为"], ["了"], ["你"], ["我"], ["愿"], ["意"], ["热"], ["爱"], ["整"], ["个"], ["世"], ["界"], ["是"], ["谁"], ["的"], ["作"], ["品"], [], [], ["郭"], ["虎"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["影"], [], [], ["为"], ["了"], ["你"], ["我"], ["愿"], ["意"], ["热"], ["爱"], ["整"], ["个"], ["世"], ["界"], ["是"], ["谁"], ["导"], ["演"], ["的"], [], [], ["灭"], ["罪"], ["师"], ["的"], ["导"], ["演"], ["是"], ["谁"], [], [], ["杨"], ["苗"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["视"], ["剧"], [], [], ["灭"], ["罪"], ["师"], ["是"], ["谁"], ["导"], ["演"], ["的"], [], [], ["五"], ["百"], ["导"], ["演"], ["了"], ["哪"], ["些"], ["电"], ["视"], ["剧"], [], [], ["穆"], ["念"], ["慈"], ["的"], ["丈"], ["夫"], ["是"], ["谁"], [], [], ["杨"], ["康"], ["的"], ["配"], ["偶"], ["是"], ["谁"]], "intentions": ["ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_books", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_director", "ask_author", "ask_books", "ask_author", "ask_author", "ask_director", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_author", "ask_author", "ask_books", "ask_author", "ask_author", "ask_husband", "ask_author", "ask_husband"], "slot_labels": [["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["O"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_book"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["O"], ["B_name"], ["O"], ["O"], [], [], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"], ["B_name"], [], [], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_book"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["O"], ["B_name"], ["B_name"], [], [], ["O"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], [], [], ["O"], ["O"], ["B_name"], ["B_name"], ["O"], ["O"], ["B_name"], ["B_name"], [], [], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], [], [], ["B_name"], ["B_name"], ["B_name"], ["B_name"], ["O"], ["B_name"], ["O"]]}\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"55abd859-46f9-4210-ac45-669a3cec5dcd\",\"inferenceTime\":\"2021-09-24T07:27:23Z\"},\"eventVersion\":\"0\"}\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "traffic = S3Downloader.read_file(f\"s3://{data_capture_bucket}/{capture_files[0]}\")\n",
    "traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_input_data = json.loads(traffic)['captureData']['endpointInput']['data']\n",
    "endpoint_output_data = json.loads(traffic)['captureData']['endpointOutput']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode payload with base64 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'伊坂幸太郎写了哪些书\\n\\nONE PIECE総集编 THE FIRST LOG是谁写的\\n\\n高效管理Windows网络/Win32 Perl应用之道的作者是谁\\n\\nDave写了什么书\\n\\n洪荒之武道是谁写的\\n\\n玄黄真人写了哪些书\\n\\n风景景观工程体系化是谁的作品\\n\\n微知汇：万物简史的作者是谁\\n\\n孽阳的作者是谁\\n\\n茅月写了哪些书\\n\\n未来娱乐系统是谁写的\\n\\n小僧不敲木鱼有什么著作\\n\\n金装四大才子是谁的作品\\n\\n罗永贤导演了哪些电影\\n\\n为了你我愿意热爱整个世界是谁的作品\\n\\n郭虎导演了哪些电影\\n\\n为了你我愿意热爱整个世界是谁导演的\\n\\n灭罪师的导演是谁\\n\\n杨苗导演了哪些电视剧\\n\\n灭罪师是谁导演的\\n\\n五百导演了哪些电视剧\\n\\n穆念慈的丈夫是谁\\n\\n杨康的配偶是谁'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "base64.b64decode(endpoint_input_data).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"text\": [[\"伊\"], [\"坂\"], [\"幸\"], [\"太\"], [\"郎\"], [\"写\"], [\"了\"], [\"哪\"], [\"些\"], [\"书\"], [], [], [\"O\"], [\"N\"], [\"E\"], [], [\"P\"], [\"I\"], [\"E\"], [\"C\"], [\"E\"], [\"総\"], [\"集\"], [\"编\"], [], [\"T\"], [\"H\"], [\"E\"], [], [\"F\"], [\"I\"], [\"R\"], [\"S\"], [\"T\"], [], [\"L\"], [\"O\"], [\"G\"], [\"是\"], [\"谁\"], [\"写\"], [\"的\"], [], [], [\"高\"], [\"效\"], [\"管\"], [\"理\"], [\"W\"], [\"i\"], [\"n\"], [\"d\"], [\"o\"], [\"w\"], [\"s\"], [\"网\"], [\"络\"], [\"/\"], [\"W\"], [\"i\"], [\"n\"], [\"3\"], [\"2\"], [], [\"P\"], [\"e\"], [\"r\"], [\"l\"], [\"应\"], [\"用\"], [\"之\"], [\"道\"], [\"的\"], [\"作\"], [\"者\"], [\"是\"], [\"谁\"], [], [], [\"D\"], [\"a\"], [\"v\"], [\"e\"], [\"写\"], [\"了\"], [\"什\"], [\"么\"], [\"书\"], [], [], [\"洪\"], [\"荒\"], [\"之\"], [\"武\"], [\"道\"], [\"是\"], [\"谁\"], [\"写\"], [\"的\"], [], [], [\"玄\"], [\"黄\"], [\"真\"], [\"人\"], [\"写\"], [\"了\"], [\"哪\"], [\"些\"], [\"书\"], [], [], [\"风\"], [\"景\"], [\"景\"], [\"观\"], [\"工\"], [\"程\"], [\"体\"], [\"系\"], [\"化\"], [\"是\"], [\"谁\"], [\"的\"], [\"作\"], [\"品\"], [], [], [\"微\"], [\"知\"], [\"汇\"], [\"：\"], [\"万\"], [\"物\"], [\"简\"], [\"史\"], [\"的\"], [\"作\"], [\"者\"], [\"是\"], [\"谁\"], [], [], [\"孽\"], [\"阳\"], [\"的\"], [\"作\"], [\"者\"], [\"是\"], [\"谁\"], [], [], [\"茅\"], [\"月\"], [\"写\"], [\"了\"], [\"哪\"], [\"些\"], [\"书\"], [], [], [\"未\"], [\"来\"], [\"娱\"], [\"乐\"], [\"系\"], [\"统\"], [\"是\"], [\"谁\"], [\"写\"], [\"的\"], [], [], [\"小\"], [\"僧\"], [\"不\"], [\"敲\"], [\"木\"], [\"鱼\"], [\"有\"], [\"什\"], [\"么\"], [\"著\"], [\"作\"], [], [], [\"金\"], [\"装\"], [\"四\"], [\"大\"], [\"才\"], [\"子\"], [\"是\"], [\"谁\"], [\"的\"], [\"作\"], [\"品\"], [], [], [\"罗\"], [\"永\"], [\"贤\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"影\"], [], [], [\"为\"], [\"了\"], [\"你\"], [\"我\"], [\"愿\"], [\"意\"], [\"热\"], [\"爱\"], [\"整\"], [\"个\"], [\"世\"], [\"界\"], [\"是\"], [\"谁\"], [\"的\"], [\"作\"], [\"品\"], [], [], [\"郭\"], [\"虎\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"影\"], [], [], [\"为\"], [\"了\"], [\"你\"], [\"我\"], [\"愿\"], [\"意\"], [\"热\"], [\"爱\"], [\"整\"], [\"个\"], [\"世\"], [\"界\"], [\"是\"], [\"谁\"], [\"导\"], [\"演\"], [\"的\"], [], [], [\"灭\"], [\"罪\"], [\"师\"], [\"的\"], [\"导\"], [\"演\"], [\"是\"], [\"谁\"], [], [], [\"杨\"], [\"苗\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"视\"], [\"剧\"], [], [], [\"灭\"], [\"罪\"], [\"师\"], [\"是\"], [\"谁\"], [\"导\"], [\"演\"], [\"的\"], [], [], [\"五\"], [\"百\"], [\"导\"], [\"演\"], [\"了\"], [\"哪\"], [\"些\"], [\"电\"], [\"视\"], [\"剧\"], [], [], [\"穆\"], [\"念\"], [\"慈\"], [\"的\"], [\"丈\"], [\"夫\"], [\"是\"], [\"谁\"], [], [], [\"杨\"], [\"康\"], [\"的\"], [\"配\"], [\"偶\"], [\"是\"], [\"谁\"]], \"intentions\": [\"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_director\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_author\", \"ask_author\", \"ask_books\", \"ask_author\", \"ask_author\", \"ask_husband\", \"ask_author\", \"ask_husband\"], \"slot_labels\": [[\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_book\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"O\"], [\"B_name\"], [\"O\"], [\"O\"], [], [], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_book\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [], [], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"O\"], [\"B_name\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [], [], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"B_name\"], [\"O\"], [\"B_name\"], [\"O\"]]}'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.b64decode(endpoint_output_data).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor SageMaker endpoints\n",
    "\n",
    "There are mainly data quality monitoring and model quality monitoring, in which:\n",
    "\n",
    "- data quality monitoring captures inference input, and compares data statistics like min, max with a baseline created from dataset [[Monitor Data Quality](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html)]\n",
    "- model quality monitoring monitors the performance of a model by comparing the predictions that the model makes with the actual ground truth labels that the model attempts to predict. [[Monitor Model Quality](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html)]\n",
    "\n",
    "Data quality is only applicapable for tabular data, therefore **not suitable** for this question understanding use case. Here we implement a quality monitoring for model quality.\n",
    "\n",
    "Reference:\n",
    "- AWS Doc: [Amazon SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)\n",
    "- SageMaker Doc: [Amazon SageMaker Model Monitor](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.html)\n",
    "- [AWS Workshop: Model Monitor](https://sagemaker-immersionday.workshop.aws/lab4/monitoring.html)\n",
    "- [Create a Model Quality Baseline](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-baseline.html)\n",
    "\n",
    "### Create a Model Quality Baseline\n",
    "\n",
    "1.  Create an instance of the ModelQualityMonitor class. \n",
    "\n",
    "Check SageMaker ModelQualityMonitor API: [Doc](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output_bucket = 'sm-nlp-data'\n",
    "baseline_job_name = \"QABaseLineJob1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a baseline dataset in JSON with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/test/seq.in') as f:\n",
    "    x_input = f.readlines()\n",
    "    x_input = [x.strip() for x in x_input]\n",
    "with open('processed/test/label') as f:\n",
    "    y_output = f.readlines()\n",
    "    y_output = [y.strip() for y in y_output]\n",
    "with open('processed/test/seq.out') as f:\n",
    "    seq_output = f.readlines()\n",
    "    seq_output = [seq.strip().split() for seq in seq_output]\n",
    "    \n",
    "assert len(predicted_cls) == len(x_input), f\"predicted label should have the same length with input sequence {len(predicted_cls)}!={len(x_input)}\"\n",
    "    \n",
    "val_dataset = {\n",
    "    'seq_in': x_input,\n",
    "    'seq_out': seq_output,\n",
    "    'predicted_label': predicted_cls\n",
    "    'label': y_output\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_dataset.json', 'w') as f:\n",
    "    json.dump(val_dataset, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now call the suggest_baseline method of the ModelQualityMonitor object to run a baseline job. We need a baseline dataset that contains both predictions and labels stored in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  QABaseLineJob1\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-093729152554/model-monitor/baselining/QABaseLineJob1/input/baseline_dataset_input', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sm-nlp-data/QABaseLineJob1/', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "job = model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_job_name,\n",
    "    baseline_dataset='./val_dataset.json', # The S3 location of the validation dataset.\n",
    "    dataset_format=DatasetFormat.json(lines=False), # Whether the file should be read as a json object per line\n",
    "    output_s3_uri = f\"s3://{baseline_output_bucket}/{baseline_job_name}/\", # The S3 location to store the results.\n",
    "    problem_type='MulticlassClassification',\n",
    "    inference_attribute= \"predicted_label\", # The column in the dataset that contains predictions.\n",
    "    ground_truth_attribute= \"label\" # The column in the dataset that contains ground truth labels.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34m2021-09-26 08:19:15.760485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:15.760513: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:17.137553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:17.137584: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:17.137604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-102-187.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:17.137819: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,493 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:093729152554:processing-job/qabaselinejob1', 'ProcessingJobName': 'QABaseLineJob1', 'Environment': {'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"json\": {\"lines\": false}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'label', 'output_path': '/opt/ml/processing/output', 'problem_type': 'MulticlassClassification', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-093729152554/model-monitor/baselining/QABaseLineJob1/input/baseline_dataset_input', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sm-nlp-data/QABaseLineJob1/', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.4xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,493 - __main__ - INFO - Current Environment:{'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"json\": {\"lines\": false}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'label', 'output_path': '/opt/ml/processing/output', 'problem_type': 'MulticlassClassification', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,493 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"json\": {\"lines\": false}}, \"output_path\": \"/opt/ml/processing/output\", \"analysis_type\": \"MODEL_QUALITY\", \"problem_type\": \"MulticlassClassification\", \"inference_attribute\": \"label\", \"probability_attribute\": null, \"ground_truth_attribute\": \"label\", \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false}\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,493 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,494 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,550 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,551 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,551 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,558 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,558 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,558 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,978 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.102.187\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-\u001b[0m\n",
      "\u001b[34m3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_302\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,985 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:18,988 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-427f04a7-5793-4eaa-bf8a-17107a275e05\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,375 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,388 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,389 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,391 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,396 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,396 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,396 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,396 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,432 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,443 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,443 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,451 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,451 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Sep 26 08:19:19\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,453 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,453 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,454 INFO util.GSet: 2.0% max memory 13.5 GB = 276.9 MB\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,454 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,544 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,548 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,548 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,548 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,548 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,549 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,549 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,549 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,549 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,549 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,549 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,549 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,580 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,580 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,580 INFO util.GSet: 1.0% max memory 13.5 GB = 138.5 MB\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,580 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,791 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,791 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,791 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,792 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,797 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,800 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,800 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,801 INFO util.GSet: 0.25% max memory 13.5 GB = 34.6 MB\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,801 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,808 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,808 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,808 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,811 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,812 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,813 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,813 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,813 INFO util.GSet: 0.029999999329447746% max memory 13.5 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,813 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,833 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1110553342-10.2.102.187-1632644359828\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,844 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,851 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,945 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,956 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,959 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.102.187\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:19,980 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:22,035 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:22,036 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:24,091 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:24,091 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:26,149 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:26,149 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:28,204 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:28,204 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:30,258 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:30,258 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:40,269 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  Main:28 - Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  Main:31 - Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  FileUtil:66 - Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SparkContext:54 - Running Spark version 2.3.1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SparkContext:54 - Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 37777.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SparkEnv:54 - Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  SparkEnv:54 - Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-a65bd1f0-14c7-4ebd-9a0c-46a20f3d9338\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:41 INFO  MemoryStore:54 - MemoryStore started with capacity 1458.6 MB\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  SparkContext:54 - Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.102.187:37777/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1632644382057\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  RMProxy:133 - Connecting to ResourceManager at /10.2.102.187:8032\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  Client:54 - Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  Configuration:2636 - resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  ResourceUtils:427 - Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (62950 MB per container)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  Client:54 - Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  Client:54 - Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:42 INFO  Client:54 - Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:43 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:45 INFO  Client:54 - Uploading resource file:/tmp/spark-6df13eec-6c5e-478f-a344-42bd73c3c511/__spark_libs__522969647772394071.zip -> hdfs://10.2.102.187/user/root/.sparkStaging/application_1632644365150_0001/__spark_libs__522969647772394071.zip\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  Client:54 - Uploading resource file:/tmp/spark-6df13eec-6c5e-478f-a344-42bd73c3c511/__spark_conf__190761126152039612.zip -> hdfs://10.2.102.187/user/root/.sparkStaging/application_1632644365150_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  Client:54 - Submitting application application_1632644365150_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  YarnClientImpl:310 - Submitted application application_1632644365150_0001\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:46 INFO  SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1632644365150_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:47 INFO  Client:54 - Application report for application_1632644365150_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:47 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1632644386553\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1632644365150_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:48 INFO  Client:54 - Application report for application_1632644365150_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:49 INFO  Client:54 - Application report for application_1632644365150_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:50 INFO  YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1632644365150_0001), /proxy/application_1632644365150_0001\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:50 INFO  Client:54 - Application report for application_1632644365150_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:50 INFO  YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  Client:54 - Application report for application_1632644365150_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.102.187\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: 0\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1632644386553\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1632644365150_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  YarnClientSchedulerBackend:54 - Application application_1632644365150_0001 has started running.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39799.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  NettyBlockTransferService:54 - Server created on 10.2.102.187:39799\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.2.102.187, 39799, None)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.2.102.187:39799 with 1458.6 MB RAM, BlockManagerId(driver, 10.2.102.187, 39799, None)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.2.102.187, 39799, None)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.2.102.187, 39799, None)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:51 INFO  log:192 - Logging initialized @11324ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:52 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.102.187:34724) with ID 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:19:52 INFO  BlockManagerMasterEndpoint:54 - Registering block manager algo-1:45341 with 23.9 GB RAM, BlockManagerId(1, algo-1, 45341, None)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:12 INFO  YarnClientSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:12 WARN  SparkContext:66 - Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:12 INFO  DatasetReader:91 - Files to process:List(file:///opt/ml/processing/input/baseline_dataset_input/val_dataset.json)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:12 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.3.1/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:12 INFO  SharedState:54 - Warehouse path is 'file:/usr/spark-2.3.1/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:12 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  CodeGenerator:54 - Code generated in 138.159258 ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 429.4 KB, free 1458.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.2.102.187:39799 (size: 38.3 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  SparkContext:54 - Created broadcast 0 from json at DatasetReader.scala:52\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  SparkContext:54 - Starting job: json at DatasetReader.scala:52\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  DAGScheduler:54 - Got job 0 (json at DatasetReader.scala:52) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (json at DatasetReader.scala:52)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  DAGScheduler:54 - Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  DAGScheduler:54 - Missing parents: List()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at DatasetReader.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.2.102.187:39799 (size: 5.1 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at DatasetReader.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:13 INFO  YarnScheduler:54 - Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:14 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8347 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:14 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on algo-1:45341 (size: 5.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on algo-1:45341 (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 1436 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  YarnScheduler:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - ResultStage 0 (json at DatasetReader.scala:52) finished in 1.478 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Job 0 finished: json at DatasetReader.scala:52, took 1.511012 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  FileSourceStrategy:54 - Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  CodeGenerator:54 - Code generated in 18.2388 ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  CodeGenerator:54 - Code generated in 11.887289 ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 429.4 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.2.102.187:39799 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  SparkContext:54 - Created broadcast 2 from count at MulticlassClassificationAnalyzer.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  SparkContext:54 - Starting job: count at MulticlassClassificationAnalyzer.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Registering RDD 6 (count at MulticlassClassificationAnalyzer.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Got job 1 (count at MulticlassClassificationAnalyzer.scala:53) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (count at MulticlassClassificationAnalyzer.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at MulticlassClassificationAnalyzer.scala:53), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 11.7 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 10.2.102.187:39799 (size: 6.1 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at MulticlassClassificationAnalyzer.scala:53) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  YarnScheduler:54 - Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on algo-1:45341 (size: 6.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on algo-1:45341 (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 161 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  YarnScheduler:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - ShuffleMapStage 1 (count at MulticlassClassificationAnalyzer.scala:53) finished in 0.177 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - waiting: Set(ResultStage 2)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Submitting ResultStage 2 (MapPartitionsRDD[9] at count at MulticlassClassificationAnalyzer.scala:53), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1457.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 10.2.102.187:39799 (size: 3.8 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at MulticlassClassificationAnalyzer.scala:53) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  YarnScheduler:54 - Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on algo-1:45341 (size: 3.8 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:15 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 80 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  YarnScheduler:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - ResultStage 2 (count at MulticlassClassificationAnalyzer.scala:53) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Job 1 finished: count at MulticlassClassificationAnalyzer.scala:53, took 0.286171 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  FileSourceStrategy:54 - Output Data Schema: struct<label: array<string>>\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  CodeGenerator:54 - Code generated in 21.823143 ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 429.4 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on 10.2.102.187:39799 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  SparkContext:54 - Created broadcast 5 from rdd at ModelQualityAnalyzer.scala:296\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on 10.2.102.187:39799 in memory (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on algo-1:45341 in memory (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned shuffle 0\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on 10.2.102.187:39799 in memory (size: 6.1 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on algo-1:45341 in memory (size: 6.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  SparkContext:54 - Starting job: collectAsMap at ModelQualityAnalyzer.scala:296\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 0\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Registering RDD 14 (rdd at ModelQualityAnalyzer.scala:296)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Got job 2 (collectAsMap at ModelQualityAnalyzer.scala:296) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 10.2.102.187:39799 in memory (size: 3.8 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (collectAsMap at ModelQualityAnalyzer.scala:296)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at rdd at ModelQualityAnalyzer.scala:296), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on algo-1:45341 in memory (size: 3.8 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 10.2.102.187:39799 in memory (size: 5.1 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on algo-1:45341 in memory (size: 5.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on 10.2.102.187:39799 in memory (size: 38.3 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on algo-1:45341 in memory (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  ContextCleaner:54 - Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 19.6 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.2 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 10.2.102.187:39799 (size: 9.2 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at rdd at ModelQualityAnalyzer.scala:296) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  YarnScheduler:54 - Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:16 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on algo-1:45341 (size: 9.2 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on algo-1:45341 (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 906 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - ShuffleMapStage 3 (rdd at ModelQualityAnalyzer.scala:296) finished in 0.937 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - waiting: Set(ResultStage 4)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting ResultStage 4 (ShuffledRDD[15] at reduceByKey at ModelQualityAnalyzer.scala:296), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 3.2 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1965.0 B, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 10.2.102.187:39799 (size: 1965.0 B, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (ShuffledRDD[15] at reduceByKey at ModelQualityAnalyzer.scala:296) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on algo-1:45341 (size: 1965.0 B, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 1 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - ResultStage 4 (collectAsMap at ModelQualityAnalyzer.scala:296) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Job 2 finished: collectAsMap at ModelQualityAnalyzer.scala:296, took 0.992655 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceStrategy:54 - Output Data Schema: struct<label: array<string>>\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  CodeGenerator:54 - Code generated in 14.342416 ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 429.4 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on 10.2.102.187:39799 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 8 from rdd at MulticlassClassificationAnalyzer.scala:149\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceStrategy:54 - Output Data Schema: struct<label: array<string>>\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  CodeGenerator:54 - Code generated in 10.563296 ms\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 429.4 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on 10.2.102.187:39799 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 9 from rdd at MulticlassClassificationAnalyzer.scala:153\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:48\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Registering RDD 26 (map at MulticlassMetrics.scala:45)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Got job 3 (collectAsMap at MulticlassMetrics.scala:48) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Final stage: ResultStage 6 (collectAsMap at MulticlassMetrics.scala:48)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 5 (MapPartitionsRDD[26] at map at MulticlassMetrics.scala:45), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 18.1 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.1 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on 10.2.102.187:39799 (size: 9.1 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[26] at map at MulticlassMetrics.scala:45) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on algo-1:45341 (size: 9.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on algo-1:45341 (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 411 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - ShuffleMapStage 5 (map at MulticlassMetrics.scala:45) finished in 0.428 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - waiting: Set(ResultStage 6)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting ResultStage 6 (ShuffledRDD[27] at reduceByKey at MulticlassMetrics.scala:47), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on 10.2.102.187:39799 (size: 1975.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 11 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 6 (ShuffledRDD[27] at reduceByKey at MulticlassMetrics.scala:47) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on algo-1:45341 (size: 1975.0 B, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 2 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - ResultStage 6 (collectAsMap at MulticlassMetrics.scala:48) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Job 3 finished: collectAsMap at MulticlassMetrics.scala:48, took 0.465322 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Starting job: countByValue at MulticlassMetrics.scala:42\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Registering RDD 30 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Got job 4 (countByValue at MulticlassMetrics.scala:42) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 7 (MapPartitionsRDD[30] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 18.7 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 9.2 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on 10.2.102.187:39799 (size: 9.2 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 12 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[30] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on algo-1:45341 (size: 9.2 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - ShuffleMapStage 7 (countByValue at MulticlassMetrics.scala:42) finished in 0.044 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - waiting: Set(ResultStage 8)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting ResultStage 8 (ShuffledRDD[31] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 1947.0 B, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on 10.2.102.187:39799 (size: 1947.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 8 (ShuffledRDD[31] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Adding task set 8.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on algo-1:45341 (size: 1947.0 B, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 3 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - ResultStage 8 (countByValue at MulticlassMetrics.scala:42) finished in 0.036 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Job 4 finished: countByValue at MulticlassMetrics.scala:42, took 0.084772 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Registering RDD 32 (map at MulticlassMetrics.scala:50)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Got job 5 (collectAsMap at MulticlassMetrics.scala:53) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (collectAsMap at MulticlassMetrics.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 9 (MapPartitionsRDD[32] at map at MulticlassMetrics.scala:50), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 18.1 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.1 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on 10.2.102.187:39799 (size: 9.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  SparkContext:54 - Created broadcast 14 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[32] at map at MulticlassMetrics.scala:50) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  YarnScheduler:54 - Adding task set 9.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:17 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on algo-1:45341 (size: 9.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ShuffleMapStage 9 (map at MulticlassMetrics.scala:50) finished in 0.033 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ResultStage 10 (ShuffledRDD[33] at reduceByKey at MulticlassMetrics.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 3.2 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 1974.0 B, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on 10.2.102.187:39799 (size: 1974.0 B, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 15 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[33] at reduceByKey at MulticlassMetrics.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on algo-1:45341 (size: 1974.0 B, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 4 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ResultStage 10 (collectAsMap at MulticlassMetrics.scala:53) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Job 5 finished: collectAsMap at MulticlassMetrics.scala:53, took 0.064544 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:48\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Registering RDD 34 (map at MulticlassMetrics.scala:45)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Got job 6 (collectAsMap at MulticlassMetrics.scala:48) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Final stage: ResultStage 12 (collectAsMap at MulticlassMetrics.scala:48)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 11 (MapPartitionsRDD[34] at map at MulticlassMetrics.scala:45), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 17.2 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on 10.2.102.187:39799 (size: 8.9 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 16 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[34] at map at MulticlassMetrics.scala:45) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 11.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 11, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on algo-1:45341 (size: 8.9 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on algo-1:45341 (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 11) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ShuffleMapStage 11 (map at MulticlassMetrics.scala:45) finished in 0.057 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - waiting: Set(ResultStage 12)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ResultStage 12 (ShuffledRDD[35] at reduceByKey at MulticlassMetrics.scala:47), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 3.2 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 1973.0 B, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on 10.2.102.187:39799 (size: 1973.0 B, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 17 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 12 (ShuffledRDD[35] at reduceByKey at MulticlassMetrics.scala:47) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 12.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 12, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on algo-1:45341 (size: 1973.0 B, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 5 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 12) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ResultStage 12 (collectAsMap at MulticlassMetrics.scala:48) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Job 6 finished: collectAsMap at MulticlassMetrics.scala:48, took 0.086749 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Starting job: countByValue at MulticlassMetrics.scala:42\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Registering RDD 38 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Got job 7 (countByValue at MulticlassMetrics.scala:42) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (countByValue at MulticlassMetrics.scala:42)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 13 (MapPartitionsRDD[38] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 17.7 KB, free 1457.1 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 9.1 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on 10.2.102.187:39799 (size: 9.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 18 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[38] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 13.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 13, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on algo-1:45341 (size: 9.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 13) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ShuffleMapStage 13 (countByValue at MulticlassMetrics.scala:42) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - waiting: Set(ResultStage 14)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ResultStage 14 (ShuffledRDD[39] at countByValue at MulticlassMetrics.scala:42), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 3.2 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 1947.0 B, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on 10.2.102.187:39799 (size: 1947.0 B, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 19 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 14 (ShuffledRDD[39] at countByValue at MulticlassMetrics.scala:42) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 14.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 14, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on algo-1:45341 (size: 1947.0 B, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 6 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 14) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ResultStage 14 (countByValue at MulticlassMetrics.scala:42) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Job 7 finished: countByValue at MulticlassMetrics.scala:42, took 0.063148 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Starting job: collectAsMap at MulticlassMetrics.scala:53\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Registering RDD 40 (map at MulticlassMetrics.scala:50)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Got job 8 (collectAsMap at MulticlassMetrics.scala:53) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Final stage: ResultStage 16 (collectAsMap at MulticlassMetrics.scala:53)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 15 (MapPartitionsRDD[40] at map at MulticlassMetrics.scala:50), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 17.2 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.9 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on 10.2.102.187:39799 (size: 8.9 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 20 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[40] at map at MulticlassMetrics.scala:50) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 15.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 15, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on algo-1:45341 (size: 8.9 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 15) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ShuffleMapStage 15 (map at MulticlassMetrics.scala:50) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - waiting: Set(ResultStage 16)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ResultStage 16 (ShuffledRDD[41] at reduceByKey at MulticlassMetrics.scala:52), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_21 stored as values in memory (estimated size 3.2 KB, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_21_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1457.0 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on 10.2.102.187:39799 (size: 1975.0 B, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 21 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 16 (ShuffledRDD[41] at reduceByKey at MulticlassMetrics.scala:52) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 16.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 16, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on algo-1:45341 (size: 1975.0 B, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 7 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 16) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ResultStage 16 (collectAsMap at MulticlassMetrics.scala:53) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Job 8 finished: collectAsMap at MulticlassMetrics.scala:53, took 0.065043 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  ModelQualityAnalyzer$:198 - Not enough samples to compute standard deviation, skipping\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  FileSourceStrategy:54 - Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_22 stored as values in memory (estimated size 429.4 KB, free 1456.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_22_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1456.6 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on 10.2.102.187:39799 (size: 38.3 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 22 from count at ModelQualityAnalyzer.scala:137\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Starting job: count at ModelQualityAnalyzer.scala:137\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Registering RDD 44 (count at ModelQualityAnalyzer.scala:137)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Got job 9 (count at ModelQualityAnalyzer.scala:137) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Final stage: ResultStage 18 (count at ModelQualityAnalyzer.scala:137)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 17 (MapPartitionsRDD[44] at count at ModelQualityAnalyzer.scala:137), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_23 stored as values in memory (estimated size 11.7 KB, free 1456.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1456.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on 10.2.102.187:39799 (size: 6.1 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 23 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[44] at count at ModelQualityAnalyzer.scala:137) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 17.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 17.0 (TID 17, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on algo-1:45341 (size: 6.1 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on algo-1:45341 (size: 38.3 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 17.0 (TID 17) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ShuffleMapStage 17 (count at ModelQualityAnalyzer.scala:137) finished in 0.044 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - waiting: Set(ResultStage 18)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting ResultStage 18 (MapPartitionsRDD[47] at count at ModelQualityAnalyzer.scala:137), which has no missing parents\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_24 stored as values in memory (estimated size 7.4 KB, free 1456.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1456.5 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on 10.2.102.187:39799 (size: 3.8 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Created broadcast 24 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[47] at count at ModelQualityAnalyzer.scala:137) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Adding task set 18.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Starting task 0.0 in stage 18.0 (TID 18, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on algo-1:45341 (size: 3.8 KB, free: 23.9 GB)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 8 to 10.2.102.187:34724\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  TaskSetManager:54 - Finished task 0.0 in stage 18.0 (TID 18) in 19 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnScheduler:54 - Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - ResultStage 18 (count at ModelQualityAnalyzer.scala:137) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  DAGScheduler:54 - Job 9 finished: count at ModelQualityAnalyzer.scala:137, took 0.073010 s\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  CWMetricsWriterImpl$:24 - Cloudwatch metric publishing is disabled, not publishing metrics\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  FileUtil:29 - Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  FileUtil:29 - Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnClientSchedulerBackend:54 - Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnClientSchedulerBackend:54 - Shutting down all executors\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  YarnClientSchedulerBackend:54 - Stopped\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  MemoryStore:54 - MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManager:54 - BlockManager stopped\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  SparkContext:54 - Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  Main:65 - Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  Main:141 - Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  ShutdownHookManager:54 - Shutdown hook called\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-8864f383-d063-47a6-ab29-46a004d45d7d\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6df13eec-6c5e-478f-a344-42bd73c3c511\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18,868 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2021-09-26 08:20:18,868 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job.wait(logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the baseline job finishes, you can see the constraints that the job generated. First, get the results of the baseline job by calling the `latest_baselining_job` method of the ModelQualityMonitor object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline job suggests constraints, which are thresholds for metrics that model monitor measures. If a metric goes beyond the suggested threshold, Model Monitor reports a violation. To view the constraints that the baseline job generated, call the suggested_constraints method of the baseline job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>comparison_operator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>1</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_recall</th>\n",
       "      <td>1</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_precision</th>\n",
       "      <td>1</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_f0_5</th>\n",
       "      <td>1</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_f2</th>\n",
       "      <td>1</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   threshold comparison_operator\n",
       "accuracy                   1   LessThanThreshold\n",
       "weighted_recall            1   LessThanThreshold\n",
       "weighted_precision         1   LessThanThreshold\n",
       "weighted_f0_5              1   LessThanThreshold\n",
       "weighted_f1                1   LessThanThreshold\n",
       "weighted_f2                1   LessThanThreshold"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(baseline_job.suggested_constraints().body_dict['multiclass_classification_constraints']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule Model Quality Monitoring Jobs \n",
    "\n",
    "Check Amazon docs for setting up a scheduled monitor quality check: [Schedule Model Quality Monitoring Jobs ](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-schedule.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Ground Truth Labels and Merge Them With Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
