{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A SageMaker Workflow\n",
    "\n",
    "**This notebook only serves for test purpose, it has the same content as `pipeline.py`**\n",
    "\n",
    "Check the notebool `sagemaker-pipelines-project.ipynb` for an end-to-end pipeline.\n",
    "\n",
    "This notebook is adapted from  [workshop/10_pipeline/01_Create_SageMaker_Pipeline_BERT_Reviews.ipynb](https://github.com/data-science-on-aws/workshop/blob/master/10_pipeline/01_Create_SageMaker_Pipeline_BERT_Reviews.ipynb)\n",
    "\n",
    "The pipeline that we create follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and model registration:\n",
    "\n",
    "![A typical ML Application pipeline](https://raw.githubusercontent.com/data-science-on-aws/workshop/23135c38b601894a4bec31a4516415d97a4750cc/10_pipeline/img/pipeline-full.png)\n",
    "\n",
    "### Create SageMaker Clients and Session\n",
    "\n",
    "First, we create a new SageMaker Session in the current AWS region. We also acquire the role arn for the session.\n",
    "\n",
    "This role arn should be the execution role arn that you set up in the Prerequisites section of this notebook.\n",
    "\n",
    "**Replace the bucket below with your own bucket name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = 'sm-nlp-data'\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track the Pipeline as an `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_name' (str)\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = \"kg-pipeline-{}\".format(timestamp)\n",
    "%store pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline experiment name: kg-pipeline-1631779222\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "pipeline_experiment = Experiment.create(\n",
    "    experiment_name=pipeline_name,\n",
    "    description=\"SPO Extraction Pipeline Experiment\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "pipeline_experiment_name = pipeline_experiment.experiment_name\n",
    "print(\"Pipeline experiment name: {}\".format(pipeline_experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_experiment_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1631779222\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.trial import Trial\n",
    "\n",
    "pipeline_trial = Trial.create(\n",
    "    trial_name=\"trial-{}\".format(timestamp), experiment_name=pipeline_experiment_name, sagemaker_boto_client=sm\n",
    ")\n",
    "\n",
    "pipeline_trial_name = pipeline_trial.trial_name\n",
    "print(\"Trial name: {}\".format(pipeline_trial_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "We define Workflow Parameters by which we can parametrize our Pipeline and vary the values injected and used in Pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* `ParameterString` - representing a `str` Python type\n",
    "* `ParameterInteger` - representing an `int` Python type\n",
    "* `ParameterFloat` - representing a `float` Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow below include:\n",
    "\n",
    "* `processing_instance_type` - The `ml.*` instance type of the processing job.\n",
    "* `processing_instance_count` - The instance count of the processing job. For illustrative purposes only: 1 is the only value that makes sense here.\n",
    "* `train_instance_type` - The `ml.*` instance type of the training job.\n",
    "* `model_approval_status` - What approval status to register the trained model with for CI/CD purposes. Defaults to \"PendingManualApproval\". (NOTE: not available in service yet)\n",
    "* `input_data` - The URL location of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = ParameterString(\n",
    "    name=\"ExperimentName\",\n",
    "    default_value=pipeline_experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Processing Step for Feature Engineering](https://raw.githubusercontent.com/data-science-on-aws/workshop/23135c38b601894a4bec31a4516415d97a4750cc/10_pipeline/img/pipeline-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't have the raw data in your s3 yet, run the following cell to download the dataset and upload it to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$bucket\"\n",
    "\n",
    "wget http://dataset-bj.cdn.bcebos.com/qianyan/DuIE_2_0.zip\n",
    "aws s3 cp DuIE_2_0.zip \"s3://$1/ie-baseline/raw/DuIE_2_0.zip\"\n",
    "rm DuIE_2_0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input path: s3://sm-nlp-data/ie-baseline/raw/DuIE_2_0.zip\n",
      "output dir: s3://sm-nlp-data/ie-baseline/processed/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = \"s3://{}/ie-baseline/raw/DuIE_2_0.zip\".format(bucket)\n",
    "print('input path:', raw_input_data_s3_uri)\n",
    "processed_data_s3_uri = \"s3://{}/ie-baseline/processed/\".format(bucket)\n",
    "print('output dir:', processed_data_s3_uri)\n",
    "\n",
    "# smaller psudo dataset\n",
    "# processed_data_s3_uri = \"s3://{}/psudo/processed/\".format(bucket)\n",
    "# raw_input_data_s3_uri = \"s3://{}/psudo/DuIE_2_0.zip\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-08 11:31:26   37097755 DuIE_2_0.zip\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_input_data_s3_uri,\n",
    ")\n",
    "output_dir = ParameterString(\n",
    "    name=\"OutputData\",\n",
    "    default_value=processed_data_s3_uri,\n",
    ")\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.c5.2xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pygmentize ./preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of an `SKLearnProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "We also specify the `framework_version` we will use throughout.\n",
    "\n",
    "Note the `processing_instance_type` and `processing_instance_count` parameters that used by the processor instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={\"AWS_DEFAULT_REGION\": region},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-919ef9c611d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ProcessingInput(\n\u001b[1;32m      6\u001b[0m         \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/opt/ml/processing/ie/data/raw\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0ms3_data_distribution_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ShardedByS3Key\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processing_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"raw\",\n",
    "        source=input_data,\n",
    "        destination=\"/opt/ml/processing/ie/data/raw\",\n",
    "        s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "    )\n",
    "]\n",
    "\n",
    "processing_outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=\"train\",\n",
    "        destination = output_dir.default_value,\n",
    "        s3_upload_mode=\"EndOfJob\",\n",
    "        source=\"/opt/ml/processing/ie/data/processed\",\n",
    "    )\n",
    "]\n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name=\"Processing\",\n",
    "    code=\"preprocess.py\",\n",
    "    processor=processor,\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    job_arguments=[\n",
    "        \"--input-data\",\n",
    "        processing_inputs[0].destination, # /opt/ml/processing/ie/data/raw\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "Note the `input_data` parameters passed into `ProcessingStep` as the input data of the step itself. This input data will be used by the processor instance when it is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = ParameterString(name=\"TrainInstanceType\", default_value=\"ml.g4dn.16xlarge\")\n",
    "train_instance_count = ParameterInteger(name=\"TrainInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Hyper-Parameters\n",
    "Note that `max_seq_length` is re-used from the processing hyper-parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = ParameterInteger(name=\"Epochs\", default_value=1)\n",
    "learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.005)\n",
    "batch_size = ParameterInteger(name=\"BatchSize\", default_value=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'eval:f1', 'Regex': 'f1: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'eval:prec', 'Regex': 'precision: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'eval:recall', 'Regex': 'recall: ([0-9\\\\./]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  !pygmentize model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Debugger and Profiler\n",
    "Define Debugger Rules as described here:  https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "from sagemaker.debugger import DebuggerHookConfig\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "    s3_output_path=\"s3://{}/ie-baseline/debug\".format(bucket),\n",
    ")\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(local_path=\"/opt/ml/output/profiler/\", start_step=5, num_steps=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [ProfilerRule.sagemaker(rule_configs.ProfilerReport())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Training Step to Train a Model\n",
    "\n",
    "We configure an Estimator and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later.\n",
    "\n",
    "We also specify the model path where the models from training will be saved.\n",
    "\n",
    "Note the `train_instance_type` parameter passed may be also used and passed into other places in the pipeline. In this case, the `train_instance_type` is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "import os\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./',\n",
    "    role=role,\n",
    "    instance_type=train_instance_type, # ml.c5.4xlarge, ml.g4dn.4xlarge\n",
    "    instance_count=train_instance_count,\n",
    "    framework_version='1.8.1',\n",
    "    py_version='py3',\n",
    "    output_path=f\"s3://{bucket}/ie-baseline/outputs\",\n",
    "    code_location=f\"s3://{bucket}/ie-baseline/source/train\", # where custom code will be uploaded \n",
    "    hyperparameters={\n",
    "        'epochs': 20,\n",
    "        'use-cuda': True,\n",
    "\n",
    "    },\n",
    "    metric_definitions = metric_definitions,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    profiler_config=profiler_config,\n",
    "    rules=rules\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pipeline Step Caching\n",
    "Cache pipeline steps for a duration of time using [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601#Durations) format.  \n",
    "\n",
    "More details on SageMaker Pipeline step caching here:  https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the estimator instance to construct a `TrainingStep` as well as the `Properties` of the prior `ProcessingStep` used as input in the `TrainingStep` inputs and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to an estimator's `fit` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3Uri` of the `\"train\"`, `\"validation\"` and `\"test\"` output channel to the `TrainingStep`. The `properties` attribute of a Workflow step match the object model of the corresponding response of a describe call. These properties can be referenced as placeholder values and are resolved, or filled in, at runtime. For example, the `ProcessingStep` `properties` attribute matches the object model of the [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.workflow.properties.Properties at 0x7f33df20f080>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='Train', step_type=<StepTypeEnum.TRAINING: 'Training'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"Train\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"application/json\",\n",
    "        ),\n",
    "    },\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sagemaker.workflow.properties.Properties object at 0x7f33de644a90>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(training_step.properties.OutputDataConfig.S3OutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': '20', 'use-cuda': 'true'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Model Evaluation Step to Evaluate the Trained Model](https://raw.githubusercontent.com/data-science-on-aws/workshop/23135c38b601894a4bec31a4516415d97a4750cc/10_pipeline/img/pipeline-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we develop an evaluation script that will be specified in a Processing step that will perform the model evaluation.\n",
    "\n",
    "The evaluation script `evaluation.py` takes the trained model and the test dataset as input, and produces a JSON file containing classification evaluation metrics such as accuracy.\n",
    "\n",
    "After pipeline execution, we will examine the resulting `evaluation.json` for analysis.\n",
    "\n",
    "The evaluation script:\n",
    "\n",
    "* loads in the model\n",
    "* reads in the test data\n",
    "* issues a bunch of predictions against the test data\n",
    "* builds a classification report, including accuracy\n",
    "* saves the evaluation report to the evaluation directory\n",
    "\n",
    "Next, we create an instance of a `ScriptProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "Note the `processing_instance_type` parameter passed into the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(\n",
    "    role=role,\n",
    "    framework_version=\"0.23-1\",\n",
    "#     py_version='py3',\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={\"AWS_DEFAULT_REGION\": region},\n",
    "    max_runtime_in_seconds=7200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pygmentize evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "The `TrainingStep` and `ProcessingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) and  [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response objects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(name=\"EvaluationReport\", output_name=\"metrics\", path=\"evaluation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "evaluation_step = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    processor=evaluation_processor,\n",
    "    code=\"evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name='model',\n",
    "#             TODO replace back: training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#             source='s3://sm-nlp-data/ie-baseline/outputs/pipelines-kbq0c1plcgkk-Train-ydNFvHuBJZ/output/model.tar.gz',\n",
    "            destination=\"/opt/ml/processing/input/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name='data',\n",
    "#             source='s3://sm-nlp-data/ie-baseline/train/',\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/data\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name='source',\n",
    "#             source='s3://sm-nlp-data/ie-baseline/source/train/pytorch-training-2021-08-30-07-40-37-881/source/sourcedir.tar.gz',\n",
    "            source=training_step.arguments['HyperParameters']['sagemaker_submit_directory'][1:-1],\n",
    "            destination=\"/opt/ml/processing/input/source/train\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"metrics\", s3_upload_mode=\"EndOfJob\", source=\"/opt/ml/processing/output/metrics/\"\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--max-seq-length\",\n",
    "        \"128\",\n",
    "        \"--source-dir\",\n",
    "        \"/opt/ml/processing/input/source/train\"\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model_metrics.ModelMetrics object at 0x7f33ddeb5ba8>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Step\n",
    "\n",
    "In order to perform batch transformation using the example model, create a SageMaker model.\n",
    "\n",
    "Specifically, pass in the S3ModelArtifacts from the TrainingStep, step_train properties. The TrainingStep properties attribute matches the object model of the DescribeTrainingJob response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_instance_type = ParameterString(name=\"TransformInstanceType\", default_value=\"ml.c5.4xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.8.1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=transform_instance_type,\n",
    "    image_scope='inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model import FrameworkModel\n",
    "import time\n",
    "\n",
    "# model_s3 = 's3://sm-nlp-data/psudo/model.tar.gz'\n",
    "model_data = training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "model_name = \"transform-model-{}\".format(timestamp)\n",
    "\n",
    "model = PyTorchModel(name=model_name,\n",
    "                    model_data=model_data,\n",
    "                    framework_version='1.3.1',\n",
    "                    py_version='py3',\n",
    "                    role=role,\n",
    "                    entry_point='inference.py',\n",
    "                    source_dir='./',\n",
    "                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Supply the model input -- instance_type and accelerator_type for creating the SageMaker Model and then define the CreateModelStep passing in the inputs and the model instance defined before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "\n",
    "create_inputs = CreateModelInput(\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"CreateKgGenModel\",\n",
    "    model=model,\n",
    "    inputs=create_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Define a Transform Step to Perform Batch Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{bucket}/ie-baseline/outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to upload a test input for transform task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/psudo_transform_input.json to s3://sm-nlp-data/psudo/psudo.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/psudo_transform_input.json s3://$bucket/psudo/psudo.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data_uri = f's3://{bucket}/psudo/psudo.json'\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"KgTransform\", transformer=transformer, inputs=TransformInput(data=batch_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.workflow.properties.Properties at 0x7f33eb2ad438>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_transform.properties.TransformOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'capitalize',\n",
       " 'casefold',\n",
       " 'center',\n",
       " 'count',\n",
       " 'encode',\n",
       " 'endswith',\n",
       " 'expandtabs',\n",
       " 'find',\n",
       " 'format',\n",
       " 'format_map',\n",
       " 'index',\n",
       " 'isalnum',\n",
       " 'isalpha',\n",
       " 'isdecimal',\n",
       " 'isdigit',\n",
       " 'isidentifier',\n",
       " 'islower',\n",
       " 'isnumeric',\n",
       " 'isprintable',\n",
       " 'isspace',\n",
       " 'istitle',\n",
       " 'isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'maketrans',\n",
       " 'partition',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(step_transform.transformer.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Register Model Step to Create a Model Package\n",
    "\n",
    "Use the estimator instance specified in the training step to construct an instance of RegisterModel. The result of executing RegisterModel in a pipeline is a model package. A model package is a reusable model artifacts abstraction that packages all ingredients required for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. A model package group can be created for a specific ML business problem, and new versions of the model packages can be added to it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker pipeline so that model package versions can be added to the group for every SageMaker Pipeline run.\n",
    "\n",
    "The construction of RegisterModel is similar to an estimator instance's register method in the Python SDK.\n",
    "\n",
    "Specifically, pass in the S3ModelArtifacts from the TrainingStep, step_train properties. The TrainingStep properties attribute matches the object model of the DescribeTrainingJob response object.\n",
    "\n",
    "Note that the specific model package group name provided in this notebook can be used in the model registry and CI/CD work with SageMaker Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "deploy_instance_type = ParameterString(name=\"DeployInstanceType\", default_value=\"ml.m4.xlarge\")\n",
    "deploy_instance_count = ParameterInteger(name=\"DeployInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG-Generation-Models-1631512267\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = f\"KG-Generation-Models-{timestamp}\"\n",
    "\n",
    "print(model_package_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "#             's3://sagemaker-us-east-1-093729152554/sagemaker-scikit-learn-2021-08-30-08-55-25-686/output/metrics/evaluation.json'\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "step_register = RegisterModel(\n",
    "    name=\"KgRegisterModel\",\n",
    "    estimator=estimator,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#     model_data=model_s3,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.c5.4xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Condition Step to Check Accuracy and Conditionally Register Model\n",
    "\n",
    "Finally, we'd like to only register this model if the accuracy of the model, as determined by our evaluation step `step_eval`, exceeded some value. A `ConditionStep` allows for pipelines to support conditional execution in the pipeline DAG based on conditions of step properties. \n",
    "\n",
    "Below, we:\n",
    "\n",
    "* define a `ConditionGreaterThan` on the accuracy value found in the output of the evaluation step, `step_eval`.\n",
    "* use the condition in the list of conditions in a `ConditionStep`\n",
    "* pass the `RegisterModel` step collection into the `if_steps` of the `ConditionStep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_f1_value = ParameterFloat(name=\"MinF1Value\", default_value=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "minimum_f1_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=evaluation_step,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"f1\",\n",
    "    ),\n",
    "    right=min_f1_value,  # accuracy\n",
    ")\n",
    "\n",
    "minimum_f1_condition_step = ConditionStep(\n",
    "    name=\"F1Condition\",\n",
    "    conditions=[minimum_f1_condition],\n",
    "    if_steps=[step_register, step_create_model, step_transform],  # success, continue with model registration\n",
    "    else_steps=[],  # fail, end the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Pipeline of Parameters, Steps, and Conditions\n",
    "\n",
    "Let's tie it all up into a workflow pipeline so we can execute it, and even schedule it.\n",
    "\n",
    "A pipeline requires a `name`, `parameters`, and `steps`. Names must be unique within an `(account, region)` pair so we tack on the timestamp to the name.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline need not be in the order of execution. The SageMaker Workflow service will resolve the _data dependency_ DAG as steps the execution complete.\n",
    "* Steps must be unique to either pipeline step list or a single condition step if/else list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-46a065f0533e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     parameters=[\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_name' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        exp_name,\n",
    "        input_data,\n",
    "        output_dir,\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        \n",
    "        train_instance_type,\n",
    "        train_instance_count,\n",
    "        epochs,\n",
    "        learning_rate,\n",
    "        batch_size,\n",
    "        \n",
    "        batch_data,\n",
    "        model_approval_status,\n",
    "        deploy_instance_type,\n",
    "        deploy_instance_count,\n",
    "        transform_instance_type,\n",
    "        \n",
    "        min_f1_value\n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, minimum_f1_condition_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the Json of the pipeline definition that meets the SageMaker Workflow Pipeline DSL specification.\n",
    "\n",
    "By examining the definition, we're also confirming that the pipeline was well-defined, and that the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Let's submit our pipeline definition to the workflow service. The role passed in will be used by the workflow service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the pipeline, accepting all the default parameters.\n",
    "\n",
    "Values can also be passed into these pipeline parameters on starting of the pipeline, and will be covered later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=dict(\n",
    "#     InputData=raw_input_data_s3_uri,\n",
    "#     ProcessingInstanceCount=1,\n",
    "#     ProcessingInstanceType=\"ml.c5.2xlarge\",\n",
    "#     MaxSeqLength=64,\n",
    "#     BalanceDataset=\"True\",\n",
    "#     TrainSplitPercentage=0.9,\n",
    "#     ValidationSplitPercentage=0.05,\n",
    "#     TestSplitPercentage=0.05,\n",
    "#     FeatureStoreOfflinePrefix=\"reviews-feature-store-\" + str(timestamp),\n",
    "#     FeatureGroupName=\"reviews-feature-group-\" + str(timestamp),\n",
    "#     LearningRate=0.000012,\n",
    "#     TrainInstanceType=\"ml.c5.9xlarge\",\n",
    "#     TrainInstanceCount=1,\n",
    "#     Epochs=1,\n",
    "#     Epsilon=0.00000001,\n",
    "#     TrainBatchSize=128,\n",
    "#     ValidationBatchSize=128,\n",
    "#     TestBatchSize=128,\n",
    "#     TrainStepsPerEpoch=50,\n",
    "#     ValidationSteps=50,\n",
    "#     TestSteps=50,\n",
    "#     TrainVolumeSize=1024,\n",
    "#     UseXLA=\"True\",\n",
    "#     UseAMP=\"True\",\n",
    "#     FreezeBERTLayer=\"False\",\n",
    "#     EnableSageMakerDebugger=\"False\",\n",
    "#     EnableCheckpointing=\"False\",\n",
    "#     EnableTensorboard=\"False\",\n",
    "#     InputMode=\"File\",\n",
    "#     RunValidation=\"True\",\n",
    "#     RunTest=\"False\",\n",
    "#     RunSamplePredictions=\"False\",\n",
    "#     MinAccuracyValue=0.01,\n",
    "#     ModelApprovalStatus=\"PendingManualApproval\",\n",
    "#     DeployInstanceType=\"ml.m4.xlarge\",\n",
    "#     DeployInstanceCount=1,\n",
    ")\n",
    "\n",
    "execution = pipeline.start()\n",
    "\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Operations: examining and waiting for pipeline execution\n",
    "\n",
    "Now we describe execution instance and list the steps in the execution to find out more about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "execution_run = execution.describe()\n",
    "pprint(execution_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Execution Run as Trial to Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_run_name = execution_run[\"PipelineExecutionDisplayName\"]\n",
    "print(execution_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_arn = execution_run[\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Giving the first step time to start up\n",
    "time.sleep(30)\n",
    "\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status == \"Executing\":\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "        pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "    #        print('Executions for our pipeline...')\n",
    "    #        print(pipeline_execution_status)\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(executions_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline ^^ Above ^^ to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_arn = executions_response[0][\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Pipeline Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)\n",
    "\n",
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List All Artifacts Generated By The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name = None\n",
    "training_job_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define A Bulkload Processing Step\n",
    "\n",
    "This step transforms data into edges and nodes, and ingest it into Neptune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulkload_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.c5.2xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulkload_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.8.1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=bulkload_instance_type,\n",
    "    image_scope='inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "bulkload_processor = Processor(\n",
    "    role=role,\n",
    "    image_uri=bulkload_image_uri,\n",
    "    instance_type=bulkload_instance_type,\n",
    "    instance_count=1,\n",
    "    env={\"AWS_DEFAULT_REGION\": region},\n",
    "    sagemaker_session=sess,\n",
    "    entrypoint='bulkload.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=bulkload_instance_type,\n",
    "    instance_count=1,\n",
    "    env={\"AWS_DEFAULT_REGION\": region},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "transformed_data = 's3://sm-nlp-data/ie-baseline/outputs/psudo.json.out'\n",
    "raw_dataset = 's3://sm-nlp-data/ie-baseline/raw/DuIE_2_0.zip'\n",
    "# endpoint = 'https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182'\n",
    "endpoint = 'http://alb-neptune-test-62758122.us-east-1.elb.amazonaws.com'\n",
    "\n",
    "bulkload_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"TransformedData\",\n",
    "        source=transformed_data,\n",
    "        destination=\"/opt/ml/processing/ie/data/transformed\",\n",
    "        s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "    ),\n",
    "    ProcessingInput(\n",
    "        input_name=\"RawDataset\",\n",
    "        source=raw_dataset,\n",
    "        destination=\"/opt/ml/processing/ie/data/raw/\",\n",
    "        s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "bulkload_step = ProcessingStep(\n",
    "    name=\"BulkloadStep\",\n",
    "    code=\"bulkload.py\",\n",
    "    processor=processor,\n",
    "    inputs=bulkload_inputs,\n",
    "    job_arguments=[\n",
    "        \"--transformed-data\",\n",
    "        bulkload_inputs[0].destination, # /opt/ml/processing/ie/data/raw\n",
    "        \"--raw-dataset\",\n",
    "        bulkload_inputs[1].destination, # DuIE_2_0.zip\n",
    "        \"--bucket\",\n",
    "        bucket,\n",
    "        \"--save-prefix\",\n",
    "        'ie-baseline/outputs',\n",
    "        \"--role\",\n",
    "        role,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--neptune-endpoint\",\n",
    "        endpoint\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(bulkload_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=f\"test-bulkload-{int(time.time())}\",\n",
    "    parameters=[\n",
    "        bulkload_instance_type\n",
    "    ],\n",
    "    steps=[bulkload_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlertStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_emails = ParameterString(\n",
    "    name=\"AlertEmails\",\n",
    "    default_value=\"yuanchu@amazon.com\",\n",
    ")\n",
    "alert_phones = ParameterString(\n",
    "    name=\"AlertPhones\",\n",
    "    default_value=\"+8613121277075\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_alert(bucket, region, role, params, dependencies):\n",
    "    '''\n",
    "    params:\n",
    "        alert_emails\n",
    "        alert_phones\n",
    "    dependencies:\n",
    "        step_condition\n",
    "    '''\n",
    "    alert_emails = params['alert_emails']\n",
    "    alert_phones = params['alert_phones']\n",
    "\n",
    "    processor = SKLearnProcessor(\n",
    "        framework_version=\"0.23-1\",\n",
    "        role=role,\n",
    "        instance_type=\"ml.t3.medium\",\n",
    "        instance_count=1,\n",
    "        env={\"AWS_DEFAULT_REGION\": region},\n",
    "    )\n",
    "    \n",
    "    # TODO: get metrics from step_condition or step_evaluate\n",
    "    alert_step = ProcessingStep(\n",
    "        name=\"AlertDevTeam\",\n",
    "        code=os.path.join(BASE_DIR, 'alert.py'),\n",
    "        processor=processor,\n",
    "        job_arguments=[\n",
    "            \"--alert-topic\",\n",
    "            \"DummyTopic\",\n",
    "            \"--alert-message\",\n",
    "            \"DummyContent\",\n",
    "            \"--alert-emails\",\n",
    "            alert_emails,\n",
    "            \"--alert_phones\",\n",
    "            alert_phones\n",
    "        ],\n",
    "    )\n",
    "    return alert_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Credentials found in config file: ~/.aws/config\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "BASE_DIR = './'\n",
    "\n",
    "step_alert = get_step_alert(\n",
    "    bucket=bucket,\n",
    "    region=region,\n",
    "    role=role,\n",
    "    params={\n",
    "        'alert_emails': alert_emails,\n",
    "        'alert_phones': alert_phones\n",
    "    },\n",
    "    dependencies={\n",
    "        'step_condition': None\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=f\"test-alert-{int(time.time())}\",\n",
    "    parameters=[\n",
    "        alert_emails,\n",
    "        alert_phones\n",
    "    ],\n",
    "    steps=[step_alert],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata': {},\n",
      " 'Parameters': [],\n",
      " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
      "                              'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
      " 'Steps': [{'Arguments': {'AppSpecification': {'ContainerArguments': ['--alert-topic',\n",
      "                                                                      'DummyTopic',\n",
      "                                                                      '--alert-message',\n",
      "                                                                      'DummyContent',\n",
      "                                                                      '--alert-emails',\n",
      "                                                                      'immr.shen@gmail.com',\n",
      "                                                                      '--alert_phones',\n",
      "                                                                      '+8613121277075'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/alert.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-093729152554/sagemaker-scikit-learn-2021-09-15-09-24-56-506/input/code/alert.py'}}],\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
      "                                                                    'InstanceType': 'ml.t3.medium',\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole'},\n",
      "            'Name': 'AlertDevTeam',\n",
      "            'Type': 'Processing'}],\n",
      " 'Version': '2020-12-01'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-alert-1631697896\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-alert-1631697896/execution/dfzytx7ih020\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start()\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-alert-1631697896',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-alert-1631697896/execution/dfzytx7ih020',\n",
       " 'PipelineExecutionDisplayName': 'execution-1631697897827',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'CreationTime': datetime.datetime(2021, 9, 15, 9, 24, 57, 767000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 9, 15, 9, 24, 57, 767000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '115791eb-66aa-490f-b580-3900b06360b4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '115791eb-66aa-490f-b580-3900b06360b4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '504',\n",
       "   'date': 'Wed, 15 Sep 2021 09:24:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CreateDB Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cluster_identifier = ParameterString(name=\"NeptuneClusterIdentifier\", default_value=\"kg-neptune\")\n",
    "db_instance_suffix = ParameterString(name=\"NeptuneInstanceSuffix\", default_value=\"instance-1\")\n",
    "db_instance_class = ParameterString(name=\"NeptuneInstanceClass\", default_value=\"db.t3.medium\")\n",
    "iam_loadfroms3_role_name = ParameterString(name=\"IamLoadFromS3RoleName\", default_value=\"NeptuneLoadFromS3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_create_db(bucket, region, role, params, dependencies, properties):\n",
    "    '''\n",
    "    params:\n",
    "        db_cluster_identifier\n",
    "        db_instance_suffix\n",
    "        db_instance_class\n",
    "        iam_loadfroms3_role_name\n",
    "    dependencies:\n",
    "    properties:\n",
    "        neptune_metadata\n",
    "    '''\n",
    "    db_cluster_identifier = params['db_cluster_identifier']\n",
    "    db_instance_suffix = params['db_instance_suffix']\n",
    "    db_instance_class = params['db_instance_class']\n",
    "    iam_loadfroms3_role_name = params['iam_loadfroms3_role_name']\n",
    "    neptune_metadata = properties['neptune_metadata']\n",
    "\n",
    "    processor = SKLearnProcessor(\n",
    "        framework_version=\"0.23-1\",\n",
    "        role=role,\n",
    "        instance_type=\"ml.t3.medium\",\n",
    "        instance_count=1,\n",
    "        env={\"AWS_DEFAULT_REGION\": region},\n",
    "    )\n",
    "    \n",
    "    output_neptune_metadata_dir = \"/opt/ml/processing/output/\"\n",
    "    output_name = neptune_metadata.output_name\n",
    "    output_neptune_metadata_filename = evaluation_report.path.split('/')[-1]\n",
    "    \n",
    "    create_db_step = ProcessingStep(\n",
    "        name=\"RetrieveOrCreateNeptuneDB\",\n",
    "        code=os.path.join(BASE_DIR, 'createdb.py'),\n",
    "        processor=processor,\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=output_name, s3_upload_mode=\"EndOfJob\", source=output_neptune_metadata_dir\n",
    "            ),\n",
    "        ],\n",
    "        job_arguments=[\n",
    "            \"--db-cluster-identifier\",\n",
    "            db_cluster_identifier,\n",
    "            \"--db-instance-suffix\",\n",
    "            db_instance_suffix,\n",
    "            \"--db-instance-class\",\n",
    "            db_instance_class,\n",
    "            \"--load-from-s3-role-name\",\n",
    "            iam_loadfroms3_role_name,\n",
    "            \"--output-neptune-metadata-dir\",\n",
    "            output_neptune_metadata_dir,\n",
    "            \"--output-neptune-metadata-filename\",\n",
    "            output_neptune_metadata_filename\n",
    "        ],\n",
    "        property_files=[neptune_metadata]\n",
    "    )\n",
    "    return create_db_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "neptune_metadata = PropertyFile(name=\"NeptuneDbMetadata\", output_name=\"neptune_metadata\", path=\"neptune_metadata.json\")\n",
    "step_create_db = get_step_create_db(\n",
    "    bucket=bucket,\n",
    "    region=region,\n",
    "    role=role,\n",
    "    params={\n",
    "        'db_cluster_identifier': db_cluster_identifier,\n",
    "        'db_instance_suffix': db_instance_suffix,\n",
    "        'db_instance_class': db_instance_class,\n",
    "        'iam_loadfroms3_role_name': iam_loadfroms3_role_name\n",
    "    },\n",
    "    dependencies={},\n",
    "    properties={\n",
    "        'neptune_metadata': neptune_metadata\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=f\"test-create-db-{int(time.time())}\",\n",
    "    parameters=[\n",
    "        db_cluster_identifier,\n",
    "        db_instance_suffix,\n",
    "        db_instance_class,\n",
    "        iam_loadfroms3_role_name\n",
    "    ],\n",
    "    steps=[step_create_db],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata': {},\n",
      " 'Parameters': [{'DefaultValue': 'kg-neptune',\n",
      "                 'Name': 'NeptuneClusterIdentifier',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'instance-1',\n",
      "                 'Name': 'NeptuneInstanceSuffix',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'db.t3.medium',\n",
      "                 'Name': 'NeptuneInstanceClass',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'NeptuneLoadFromS3',\n",
      "                 'Name': 'IamLoadFromS3RoleName',\n",
      "                 'Type': 'String'}],\n",
      " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
      "                              'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
      " 'Steps': [{'Arguments': {'AppSpecification': {'ContainerArguments': ['--db-cluster-identifier',\n",
      "                                                                      {'Get': 'Parameters.NeptuneClusterIdentifier'},\n",
      "                                                                      '--db-instance-suffix',\n",
      "                                                                      {'Get': 'Parameters.NeptuneInstanceSuffix'},\n",
      "                                                                      '--db-instance-class',\n",
      "                                                                      {'Get': 'Parameters.NeptuneInstanceClass'},\n",
      "                                                                      '--load-from-s3-role-name',\n",
      "                                                                      {'Get': 'Parameters.IamLoadFromS3RoleName'},\n",
      "                                                                      '--output-neptune-metadata-dir',\n",
      "                                                                      '/opt/ml/processing/output/',\n",
      "                                                                      '--output-neptune-metadata-filename',\n",
      "                                                                      'evaluation.json'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/createdb.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-093729152554/sagemaker-scikit-learn-2021-09-16-09-07-51-807/input/code/createdb.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'neptune_metadata',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-093729152554/sagemaker-scikit-learn-2021-09-16-09-07-51-807/output/neptune_metadata'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
      "                                                                    'InstanceType': 'ml.t3.medium',\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole'},\n",
      "            'Name': 'RetrieveOrCreateNeptuneDB',\n",
      "            'PropertyFiles': [{'FilePath': 'neptune_metadata.json',\n",
      "                               'OutputName': 'neptune_metadata',\n",
      "                               'PropertyFileName': 'NeptuneDbMetadata'}],\n",
      "            'Type': 'Processing'}],\n",
      " 'Version': '2020-12-01'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-create-db-1631783268\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-create-db-1631783268/execution/yah33enfcjlu\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start()\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-create-db-1631783268', 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:093729152554:pipeline/test-create-db-1631783268/execution/yah33enfcjlu', 'PipelineExecutionDisplayName': 'execution-1631783277141', 'PipelineExecutionStatus': 'Executing', 'CreationTime': datetime.datetime(2021, 9, 16, 9, 7, 57, 7000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2021, 9, 16, 9, 7, 57, 7000, tzinfo=tzlocal()), 'CreatedBy': {}, 'LastModifiedBy': {}, 'ResponseMetadata': {'RequestId': 'dcba0fec-8700-4c15-bc90-13e222f03a5b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'dcba0fec-8700-4c15-bc90-13e222f03a5b', 'content-type': 'application/x-amz-json-1.1', 'content-length': '516', 'date': 'Thu, 16 Sep 2021 09:07:57 GMT'}, 'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
